{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03fea933",
   "metadata": {},
   "source": [
    "# Understanding the SPARQL Evaluation Metrics\n",
    "\n",
    "This document explains the four metrics used to evaluate the correctness of a predicted SPARQL query against a ground-truth standard. Each metric evaluates a different aspect of the query's output, progressing from high-level structural correctness to a stringent, content-level comparison.\n",
    "\n",
    "### 1. Arity Matching F1\n",
    "\n",
    "* ***Core Question:*** *\"Did the system identify the correct number of concepts to return?\"*\n",
    "* **How it Works:** This metric diagnoses the most basic errors by comparing the number of columns (the arity) in the predicted and oracle tables using an F1-score. It is a quick check of the output's fundamental shape, independent of column names, order, or content.\n",
    "* **Example Scenarios:**\n",
    "    * **Correct Arity:** A prediction with 2 columns vs. an oracle with 2 columns results in a perfect score of **1.0**.\n",
    "    * **Fewer Columns:** A prediction with 1 column vs. an oracle with 2 results in a score of **0.67**.\n",
    "    * **More Columns:** A prediction with 3 columns vs. an oracle with 2 results in a score of **0.8**.\n",
    "\n",
    "### 2. Entity Set F1\n",
    "\n",
    "* ***Core Question:*** *\"Did the system retrieve the correct set of entities?\"*\n",
    "* **How it Works:** This metric isolates performance on entity linking, independent of relational structure. It first finds the optimal column alignment by testing all possible permutations. Using this best-fit mapping, it then computes an F1-score on the *set of unique values* within each corresponding column.\n",
    "* **Key Detail:** Because it performs a full permutation search, this metric is insensitive to the original order of columns in the predicted query. If the prediction has fewer columns than the oracle, a complete mapping is impossible, and the score is correctly **0.0**.\n",
    "\n",
    "### 3. Row-Matching F1\n",
    "\n",
    "* ***Core Question:*** *\"Are the relationships between entities correctly constructed?\"*\n",
    "* **How it Works:** This stricter metric evaluates the structural correctness of the results. It begins by finding the optimal column alignment via permutation, just like the Entity Set F1. After establishing this best-fit mapping, a predicted row is considered a true positive only if an identical row exists in the oracle. This provides a robust measure of correctness that is flexible to variations in column naming and order.\n",
    "* **Example Scenarios:**\n",
    "    * A query that retrieves the correct entities but misses a relational constraint (e.g., a `brick:feeds` relationship) will have a low score (e.g., **0.16**), as most of the generated rows will not exist in the more constrained oracle.\n",
    "    * Because it finds the best alignment first, the score will be the same even if the predicted query has its columns in a different order.\n",
    "\n",
    "### 4. Exact-Match F1\n",
    "\n",
    "* ***Core Question:*** *\"Did the results match the oracle perfectly?\"*\n",
    "* **How it Works:** Equivalent to `Execution Accuracy` in Text-to-SQL, this metric provides a final, stringent measure of overall correctness. It requires that the column **order** be identical to the oracle. A positional mapping is created (first predicted column to first oracle column, etc.), and a row-wise F1 score is calculated based on this fixed structure. No column permutation is performed.\n",
    "* **Example Scenarios:**\n",
    "    * A query with perfect content and the same column order, but different column *names* (`?e`, `?s` vs. `?eqp`, `?sensor`), correctly receives a perfect score of **1.0**.\n",
    "    * A query with a different column order (`?s`, `?ahu` vs. `?eqp`, `?sensor`) correctly receives a score of **0.0**.\n",
    "    * A query with a different number of columns also correctly receives a score of **0.0**.\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Metric                | What it Checks                          | Strictness | Column Alignment Performed? |\n",
    "| --------------------- | --------------------------------------- | ---------- | --------------------------- |\n",
    "| **Arity Matching F1** | The number of columns.                  | Low        | No                          |\n",
    "| **Entity Set F1** | Sets of unique values within columns.   | Medium     | **Yes (Finds best match)** |\n",
    "| **Row-Matching F1** | Row-for-row content.                    | High       | **Yes (Finds best match)** |\n",
    "| **Exact-Match F1** | Column order and row-for-row content.   | Very High  | No (Assumes fixed order)    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aacec2b",
   "metadata": {},
   "source": [
    "# 1. Testing on generic sparql queries\n",
    "\n",
    "Below, we run unit tests on generic dummy query results for each metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79b7282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PASS: Arity F1: Correct SPARQL\n",
      "‚úÖ PASS: Arity F1: Incorrect SPARQL\n",
      "‚úÖ PASS: Arity F1: Correct Table\n",
      "‚úÖ PASS: Arity F1: Incorrect Table\n",
      "‚úÖ PASS: Arity F1: Both Empty\n",
      "‚úÖ PASS: Entity Set F1: Perfect Match\n",
      "‚úÖ PASS: Entity Set F1: Perfect Match (Mapped Names)\n",
      "‚úÖ PASS: Entity Set F1: Superset Pred\n",
      "‚úÖ PASS: Entity Set F1: Subset Pred\n",
      "‚úÖ PASS: Entity Set F1: No Match\n",
      "‚úÖ PASS: Row Match F1: Perfect Match\n",
      "‚úÖ PASS: Row Match F1: Perfect Match (Mapped Names)\n",
      "‚úÖ PASS: Row Match F1: Shuffled Order (Mapped Names)\n",
      "‚úÖ PASS: Row Match F1: Subset Pred\n",
      "‚úÖ PASS: Row Match F1: Partial Content Mismatch\n",
      "‚úÖ PASS: Exact Match F1: Perfect Match\n",
      "‚úÖ PASS: Exact Match F1: Shuffled Order\n",
      "‚úÖ PASS: Exact Match F1: Schema Mismatch\n",
      "‚úÖ PASS: Exact Match F1: Subset Pred\n",
      "‚úÖ PASS: Exact Match F1: Both Empty\n",
      "\n",
      "==================================================\n",
      "--- Test Summary ---\n",
      "üéâ All 20 tests passed! üéâ\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from metrics import (\n",
    "    get_arity_matching_f1,\n",
    "    get_entity_and_row_matching_f1,\n",
    "    get_exact_match_f1\n",
    ")\n",
    "\n",
    "# --- Test Runner ---\n",
    "def run_tests(test_cases):\n",
    "    \"\"\"A simple function to run test cases and report results.\"\"\"\n",
    "    passed_count = 0\n",
    "    failed_tests = []\n",
    "    \n",
    "    for test in test_cases:\n",
    "        func = test[\"func\"]\n",
    "        args = test[\"args\"]\n",
    "        expected = test[\"expected\"]\n",
    "        test_name = test[\"name\"]\n",
    "        \n",
    "        try:\n",
    "            actual = func(*args)\n",
    "            # Use a tolerance for floating point comparisons\n",
    "            if np.isclose(actual, expected):\n",
    "                print(f\"‚úÖ PASS: {test_name}\")\n",
    "                passed_count += 1\n",
    "            else:\n",
    "                failed_tests.append((test_name, expected, actual))\n",
    "                print(f\"‚ùå FAIL: {test_name} | Expected: {expected:.4f}, Got: {actual:.4f}\")\n",
    "        except Exception as e:\n",
    "            failed_tests.append((test_name, expected, f\"ERROR: {e}\"))\n",
    "            print(f\"üí• ERROR: {test_name} | Exception: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- Test Summary ---\")\n",
    "    if not failed_tests:\n",
    "        print(f\"üéâ All {passed_count} tests passed! üéâ\")\n",
    "    else:\n",
    "        print(f\"üî• {len(failed_tests)} test(s) failed: üî•\")\n",
    "        for name, exp, act in failed_tests:\n",
    "            print(f\"  - {name} | Expected: {exp}, Got: {act}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# --- Test Data ---\n",
    "gold_rows_base = [\n",
    "    {\"name\": {\"value\": \"Alice\"}, \"age\": {\"value\": \"30\"}},\n",
    "    {\"name\": {\"value\": \"Bob\"}, \"age\": {\"value\": \"25\"}},\n",
    "    {\"name\": {\"value\": \"Charlie\"}, \"age\": {\"value\": \"35\"}}\n",
    "]\n",
    "\n",
    "# --- Test Case Definitions ---\n",
    "# Helper wrappers to extract metrics from get_entity_and_row_matching_f1\n",
    "def entity_set_f1(gt, pred):\n",
    "    return get_entity_and_row_matching_f1(gt, pred)[\"entity_set_f1\"]\n",
    "\n",
    "def row_matching_f1(gt, pred):\n",
    "    return get_entity_and_row_matching_f1(gt, pred)[\"row_matching_f1\"]\n",
    "\n",
    "all_test_cases = [\n",
    "    # --- Tests for get_arity_matching_f1 ---\n",
    "    {\"name\": \"Arity F1: Correct SPARQL\", \"func\": get_arity_matching_f1, \"args\": [\"SELECT ?n ?a WHERE {}\", gold_rows_base], \"expected\": 1.0},\n",
    "    {\"name\": \"Arity F1: Incorrect SPARQL\", \"func\": get_arity_matching_f1, \"args\": [\"SELECT ?n WHERE {}\", gold_rows_base], \"expected\": 2/3},\n",
    "    {\"name\": \"Arity F1: Correct Table\", \"func\": get_arity_matching_f1, \"args\": [gold_rows_base, gold_rows_base], \"expected\": 1.0},\n",
    "    {\"name\": \"Arity F1: Incorrect Table\", \"func\": get_arity_matching_f1, \"args\": [[{\"name\": {\"value\": \"A\"}}], gold_rows_base], \"expected\": 2/3},\n",
    "    {\"name\": \"Arity F1: Both Empty\", \"func\": get_arity_matching_f1, \"args\": [[], []], \"expected\": 1.0},\n",
    "\n",
    "    # --- Tests for get_entity_set_f1 (No mapping provided) ---\n",
    "    {\"name\": \"Entity Set F1: Perfect Match\", \"func\": entity_set_f1, \"args\": [gold_rows_base, gold_rows_base], \"expected\": 1.0},\n",
    "    {\"name\": \"Entity Set F1: Perfect Match (Mapped Names)\", \"func\": entity_set_f1, \"args\": [gold_rows_base, [{\"person\": r[\"name\"], \"years\": r[\"age\"]} for r in gold_rows_base]], \"expected\": 1.0},\n",
    "    {\"name\": \"Entity Set F1: Superset Pred\", \"func\": entity_set_f1, \"args\": [gold_rows_base, gold_rows_base + [{\"name\": {\"value\": \"David\"}, \"age\": {\"value\": \"40\"}}]], \"expected\": 0.857142},\n",
    "    {\"name\": \"Entity Set F1: Subset Pred\", \"func\": entity_set_f1, \"args\": [gold_rows_base, gold_rows_base[:2]], \"expected\": 0.8},         \n",
    "    {\"name\": \"Entity Set F1: No Match\", \"func\": entity_set_f1, \"args\": [gold_rows_base, [{\"name\": {\"value\": \"Z\"}, \"age\": {\"value\": \"99\"}}]], \"expected\": 0.0},    \n",
    "\n",
    "    # --- Tests for get_row_matching_f1 (No mapping provided) ---\n",
    "    {\"name\": \"Row Match F1: Perfect Match\", \"func\": row_matching_f1, \"args\": [gold_rows_base, gold_rows_base], \"expected\": 1.0},\n",
    "    {\"name\": \"Row Match F1: Perfect Match (Mapped Names)\", \"func\": row_matching_f1, \"args\": [gold_rows_base, [{\"p\": r[\"name\"], \"a\": r[\"age\"]} for r in gold_rows_base]], \"expected\": 1.0},\n",
    "    {\"name\": \"Row Match F1: Shuffled Order (Mapped Names)\", \"func\": row_matching_f1, \"args\": [gold_rows_base, [{\"a\": r[\"age\"], \"p\": r[\"name\"]} for r in reversed(gold_rows_base)]], \"expected\": 1.0},\n",
    "    {\"name\": \"Row Match F1: Subset Pred\", \"func\": row_matching_f1, \"args\": [gold_rows_base, gold_rows_base[:1]], \"expected\": 0.5},\n",
    "    {\"name\": \"Row Match F1: Partial Content Mismatch\", \"func\": row_matching_f1, \"args\": [gold_rows_base, [{\"name\": {\"value\": \"Alice\"}, \"age\": {\"value\": \"30\"}}, {\"name\": {\"value\": \"Bob\"}, \"age\": {\"value\": \"99\"}}]], \"expected\": 0.4},\n",
    "\n",
    "    # --- Tests for get_exact_match_f1 ---\n",
    "    {\"name\": \"Exact Match F1: Perfect Match\", \"func\": get_exact_match_f1, \"args\": [gold_rows_base, gold_rows_base], \"expected\": 1.0},\n",
    "    {\"name\": \"Exact Match F1: Shuffled Order\", \"func\": get_exact_match_f1, \"args\": [gold_rows_base, [gold_rows_base[1], gold_rows_base[2], gold_rows_base[0]]], \"expected\": 1.0},\n",
    "    {\"name\": \"Exact Match F1: Schema Mismatch\", \"func\": get_exact_match_f1, \"args\": [gold_rows_base, [{\"person\": {\"value\": \"Alice\"}, \"years\": {\"value\": \"30\"}}]], \"expected\": 0.5},\n",
    "    {\"name\": \"Exact Match F1: Subset Pred\", \"func\": get_exact_match_f1, \"args\": [gold_rows_base, gold_rows_base[:2]], \"expected\": 0.8},\n",
    "    {\"name\": \"Exact Match F1: Both Empty\", \"func\": get_exact_match_f1, \"args\": [[], []], \"expected\": 1.0},\n",
    "]\n",
    "\n",
    "\n",
    "run_tests(all_test_cases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd2fbec",
   "metadata": {},
   "source": [
    "# 2. Testing on example brick queries.\n",
    "\n",
    "Below, we run tests on different cases that we can infer from these metrics using example brick queries. We are using GraphDB to host our building graph. Change the SPARQL endpoint to your GraphDB endpoint. You can also change the code to run on rdflib by running SPARQL locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de8bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Executing SPARQL Queries ---\n",
      "üîé Running query against http://Ozans-MacBook-Pro-10.local:7200/repositories/bldg11...\n",
      "‚úÖ Query successful. Found 377 results.\n",
      "üîé Running query against http://Ozans-MacBook-Pro-10.local:7200/repositories/bldg11...\n",
      "‚úÖ Query successful. Found 4268 results.\n",
      "üîé Running query against http://Ozans-MacBook-Pro-10.local:7200/repositories/bldg11...\n",
      "‚úÖ Query successful. Found 4268 results.\n",
      "üîé Running query against http://Ozans-MacBook-Pro-10.local:7200/repositories/bldg11...\n",
      "‚úÖ Query successful. Found 377 results.\n",
      "üîé Running query against http://Ozans-MacBook-Pro-10.local:7200/repositories/bldg11...\n",
      "‚úÖ Query successful. Found 388 results.\n",
      "üîé Running query against http://Ozans-MacBook-Pro-10.local:7200/repositories/bldg11...\n",
      "‚úÖ Query successful. Found 377 results.\n",
      "\n",
      "==================================================\n",
      "--- 2. Calculating Evaluation Metrics ---\n",
      "\n",
      "--- Arity Matching F1 ---\n",
      "1. Number of columns match! Value should be 1: 1.0000\n",
      "2. Predictions have more columns than GT. Value should be 0.8: 0.8000\n",
      "3. Predictions have less columns than GT. Value should be 0.667: 0.6667\n",
      "\n",
      "--- Entity Set F1 ---\n",
      "1. Column content mostly match, it should have a high value (0.94): 0.9445\n",
      "2. Column content mostly match (but opposite order of columns), it should have a high value (0.94): 0.9445\n",
      "3. Column content should match identically, it should have a perfect score of 1: 1.0000\n",
      "4. There are fewer columns in predictions, it should be zero: 0.0000\n",
      "\n",
      "--- Row Matching F1 ---\n",
      "1. Relational connection is missing. Column order is same. Thus, row_matching should have low but non zero value (0.16): 0.1623\n",
      "2. Relational connection is missing. Column order is different. Thus, row_matching should have low but non zero value (0.16): 0.1623\n",
      "3. Row content should match identically, it should have a perfect score of 1: 1.0000\n",
      "4. There are fewer columns in predictions, it should be zero: 0.0000\n",
      "\n",
      "--- Exact Match F1 ---\n",
      "1. Exact match should be low but non-zero (0.16) since order matches:  0.1623\n",
      "1. Exact match should be zero for other test cases since schema is different:  0.0000, 0.0000, 0.0000\n",
      "2. Exact match should be 1 since schema is different but order of columns are same: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Configuration ---\n",
    "\n",
    "# Set the target SPARQL endpoint\n",
    "building_name = \"bldg11\"\n",
    "SPARQL_ENDPOINT = f\"<GRAPHDB ENDPOINT>{building_name}\" # CHANGE THIS TO YOUR GRAPHDB ENDPOINT\n",
    "\n",
    "# Define the ground truth and predicted SPARQL queries\n",
    "gold_query = \"\"\"\n",
    "    PREFIX brick: <https://brickschema.org/schema/Brick#>\n",
    "    SELECT DISTINCT ?eqp ?sensor WHERE {\n",
    "        ?eqp    a                       brick:Air_Handling_Unit .\n",
    "        ?vav    a                       brick:VAV .\n",
    "        ?sensor a                       brick:Zone_Air_Temperature_Sensor .\n",
    "        ?eqp    brick:feeds             ?vav .\n",
    "        ?vav    brick:hasPoint          ?sensor .\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "predicted_query_1 = \"\"\"\n",
    "    PREFIX brick: <https://brickschema.org/schema/Brick#>\n",
    "    SELECT DISTINCT ?ahu ?s  WHERE {\n",
    "        ?ahu    a        brick:Air_Handling_Unit .\n",
    "        ?s      a        brick:Zone_Air_Temperature_Sensor .\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "predicted_query_2 = \"\"\"\n",
    "    PREFIX brick: <https://brickschema.org/schema/Brick#>\n",
    "    SELECT DISTINCT ?s ?ahu   WHERE {\n",
    "        ?ahu    a        brick:Air_Handling_Unit .\n",
    "        ?s      a        brick:Zone_Air_Temperature_Sensor .\n",
    "    }\n",
    "\"\"\"\n",
    "predicted_query_3 = \"\"\"\n",
    "    PREFIX brick: <https://brickschema.org/schema/Brick#>\n",
    "    SELECT DISTINCT ?eqp ?vav ?sensor WHERE {\n",
    "        ?eqp    a                       brick:Air_Handling_Unit .\n",
    "        ?vav    a                       brick:VAV .\n",
    "        ?sensor a                       brick:Zone_Air_Temperature_Sensor .\n",
    "        ?eqp    brick:feeds             ?vav .\n",
    "        ?vav    brick:hasPoint          ?sensor .\n",
    "    }\n",
    "\"\"\"\n",
    "predicted_query_4 = \"\"\"\n",
    "    PREFIX brick: <https://brickschema.org/schema/Brick#>\n",
    "    SELECT DISTINCT ?s   WHERE {\n",
    "        ?ahu    a        brick:Air_Handling_Unit .\n",
    "        ?s      a        brick:Zone_Air_Temperature_Sensor .\n",
    "    }\n",
    "\"\"\"\n",
    "predicted_query_5 = \"\"\"\n",
    "    PREFIX brick: <https://brickschema.org/schema/Brick#>\n",
    "    SELECT DISTINCT ?e ?s WHERE {\n",
    "        ?e    a                       brick:Air_Handling_Unit .\n",
    "        ?vav    a                       brick:VAV .\n",
    "        ?s a                       brick:Zone_Air_Temperature_Sensor .\n",
    "        ?e    brick:feeds             ?vav .\n",
    "        ?vav    brick:hasPoint          ?s .\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# --- 2. Helper Function to Run Queries ---\n",
    "\n",
    "def run_sparql_query(query: str, endpoint_url: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Executes a SPARQL query against a remote endpoint and returns the results.\n",
    "    \"\"\"\n",
    "    print(f\"üîé Running query against {endpoint_url}...\")\n",
    "    try:\n",
    "        sparql = SPARQLWrapper(endpoint_url)\n",
    "        sparql.setQuery(query)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "        bindings = results.get(\"results\", {}).get(\"bindings\", [])\n",
    "        print(f\"‚úÖ Query successful. Found {len(bindings)} results.\")\n",
    "        return bindings\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Query failed: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- 3. Main Execution and Evaluation ---\n",
    "print(\"--- 1. Executing SPARQL Queries ---\")\n",
    "gt_results = run_sparql_query(gold_query, SPARQL_ENDPOINT)\n",
    "gen_results_1 = run_sparql_query(predicted_query_1, SPARQL_ENDPOINT)\n",
    "gen_results_2 = run_sparql_query(predicted_query_2, SPARQL_ENDPOINT)\n",
    "gen_results_3 = run_sparql_query(predicted_query_3, SPARQL_ENDPOINT)\n",
    "gen_results_4 = run_sparql_query(predicted_query_4, SPARQL_ENDPOINT)\n",
    "gen_results_5 = run_sparql_query(predicted_query_5, SPARQL_ENDPOINT)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- 2. Calculating Evaluation Metrics ---\")\n",
    "\n",
    "# Metric 1: Arity Matching F1 (No alignment needed)\n",
    "print(\"\\n--- Arity Matching F1 ---\")\n",
    "arity_f1_1 = get_arity_matching_f1(predicted_query_1, gold_query)\n",
    "print(f\"1. Number of columns match! Value should be 1: {arity_f1_1:.4f}\")\n",
    "arity_f1_3 = get_arity_matching_f1(predicted_query_3, gold_query)\n",
    "print(f\"2. Predictions have more columns than GT. Value should be 0.8: {arity_f1_3:.4f}\")\n",
    "arity_f1_4 = get_arity_matching_f1(predicted_query_4, gold_query)\n",
    "print(f\"3. Predictions have less columns than GT. Value should be 0.667: {arity_f1_4:.4f}\")\n",
    "\n",
    "# Metric 2: Entity Set F1 (Performs internal alignment)\n",
    "print(\"\\n--- Entity Set F1 ---\")\n",
    "result_1 = get_entity_and_row_matching_f1(gt_results, gen_results_1)\n",
    "entity_set_f1_1 = result_1[\"entity_set_f1\"]\n",
    "row_matching_f1_1 = result_1[\"row_matching_f1\"]\n",
    "print(f\"1. Column content mostly match, it should have a high value (0.94): {entity_set_f1_1:.4f}\")\n",
    "result_2 = get_entity_and_row_matching_f1(gt_results, gen_results_2)\n",
    "entity_set_f1_2 = result_2[\"entity_set_f1\"]\n",
    "row_matching_f1_2 = result_2[\"row_matching_f1\"]\n",
    "print(f\"2. Column content mostly match (but opposite order of columns), it should have a high value (0.94): {entity_set_f1_2:.4f}\")\n",
    "result_3 = get_entity_and_row_matching_f1(gt_results, gen_results_3)\n",
    "entity_set_f1_3 = result_3[\"entity_set_f1\"]\n",
    "row_matching_f1_3 = result_3[\"row_matching_f1\"]\n",
    "print(f\"3. Column content should match identically, it should have a perfect score of 1: {entity_set_f1_3:.4f}\")\n",
    "result_4 = get_entity_and_row_matching_f1(gt_results, gen_results_4)\n",
    "entity_set_f1_4 = result_4[\"entity_set_f1\"]\n",
    "row_matching_f1_4 = result_4[\"row_matching_f1\"]\n",
    "print(f\"4. There are fewer columns in predictions, it should be zero: {entity_set_f1_4:.4f}\")\n",
    "\n",
    "# Metric 3: Row Matching F1 (Performs internal alignment)\n",
    "print(\"\\n--- Row Matching F1 ---\")\n",
    "\n",
    "print(f\"1. Relational connection is missing. Column order is same. Thus, row_matching should have low but non zero value (0.16): {row_matching_f1_1:.4f}\")\n",
    "print(f\"2. Relational connection is missing. Column order is different. Thus, row_matching should have low but non zero value (0.16): {row_matching_f1_2:.4f}\")\n",
    "print(f\"3. Row content should match identically, it should have a perfect score of 1: {row_matching_f1_3:.4f}\")\n",
    "print(f\"4. There are fewer columns in predictions, it should be zero: {row_matching_f1_4:.4f}\")\n",
    "\n",
    "# Metric 4: Exact Match F1 (No alignment needed)\n",
    "print(\"\\n--- Exact Match F1 ---\")\n",
    "exact_match_f1_1 = get_exact_match_f1(gt_results, gen_results_1)\n",
    "exact_match_f1_2 = get_exact_match_f1(gt_results, gen_results_2)\n",
    "exact_match_f1_3 = get_exact_match_f1(gt_results, gen_results_3)\n",
    "exact_match_f1_4 = get_exact_match_f1(gt_results, gen_results_4)\n",
    "exact_match_f1_5 = get_exact_match_f1(gt_results, gen_results_5)\n",
    "print(f\"1. Exact match should be low but non-zero (0.16) since order matches:  {exact_match_f1_1:.4f}\")\n",
    "print(f\"1. Exact match should be zero for other test cases since schema is different:  {exact_match_f1_2:.4f}, {exact_match_f1_3:.4f}, {exact_match_f1_4:.4f}\")\n",
    "print(f\"2. Exact match should be 1 since schema is different but order of columns are same: {exact_match_f1_5:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
