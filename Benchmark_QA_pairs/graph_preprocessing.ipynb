{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6683b487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Query ID: DFLEXLIBS_001 ---\n",
      "Description: Retrieves all available timeseries IDs related to HVAC zones, including various temperature setpoints, occupancy sensors, and VAV points.\n",
      "\n",
      "  - Question: For each HVAC zone in the building, what are the timeseries IDs for the available zone temperature sensor' points, min and max temperature setpoints, temperature setpoints (single, heating/cooling or occupied/unoccupied), occupancy sensors and commands, and any related VAV damper, discharge temperature, and reheat command points?\n",
      "    Source: human\n",
      "\n",
      "  - Question: What is the name of each HVAC zone?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: DFLEXLIBS_002 ---\n",
      "Description: Retrieves timeseries IDs for AHU supply air temperature, flow, and flow setpoint.\n",
      "\n",
      "  - Question: For each AHU that supplies a zone, what are the timeseries IDs for all available points for supply air temperature, supply air flow, and supply air flow setpoint?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: DFLEXLIBS_003 ---\n",
      "Description: Retrieves timeseries IDs for all electric and thermal power sensors in a building.\n",
      "\n",
      "  - Question: What are the timeseries IDs for the available electric power sensors and thermal power sensors in the building?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: DFLEXLIBS_004 ---\n",
      "Description: Retrieves the total area of a building.\n",
      "\n",
      "  - Question: What is the area of a building?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: DFLEXLIBS_005 ---\n",
      "Description: A templated query to retrieve timeseries IDs for electric power sensors linked to a specific scenario.\n",
      "\n",
      "  - Question: What are the timeseries IDs for electric power sensors that are linked to the scenario named '{scenario}'?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: DFLEXLIBS_006 ---\n",
      "Description: A templated query to retrieve timeseries IDs for thermal power sensors linked to a specific scenario.\n",
      "\n",
      "  - Question: What are the timeseries IDs for thermal power sensors that are linked to the scenario named '{scenario}'?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: LBNL_B59_001 ---\n",
      "Description: Identifies non-writable property labels and their ALC and MPC endpoints for each HVAC zone.\n",
      "\n",
      "  - Question: Identify each entity that is a zone, and either has a directly associated property or is indirectly connected to a property via a RoomDomainSpace through a chain of connections.\n",
      "    Source: human\n",
      "\n",
      "  - Question: For each zone (target), what are the related non-writable property labels and ALC and MPC endpoint references?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: LBNL_B59_002 ---\n",
      "Description: Identifies non-writable property labels and their ALC and MPC endpoints for each AHU labeled as 'RTU'.\n",
      "\n",
      "  - Question: Identify each entity that is an AHU, and either has a directly associated property or is indirectly connected to a property via a connectable.\n",
      "    Source: human\n",
      "\n",
      "  - Question: For each Air Handling Unit (AHU) whose label includes 'RTU', what are the related non-writable property labels and ALC and MPC endpoint references?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: LBNL_B59_003 ---\n",
      "Description: Retrieves data for reheat ARUs by identifying Water-to-Water Heat Pumps and their contained non-writable properties.\n",
      "\n",
      "  - Question: Get reheat ARU data (for UFT reheat) by identifying each entity that is a Water-to-Water Heat Pump and contains a property that observes a non-writable quantity with reference endpoints.\n",
      "    Source: human\n",
      "\n",
      "  - Question: For each Water-to-Water Heat Pump, what are the related property labels and non-writable quantities' ALC and MPC endpoint references?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: LBNL_B59_004 ---\n",
      "Description: Retrieves non-writable property labels and their ALC/MPC endpoints for Water-to-Water Heat Pumps.\n",
      "\n",
      "  - Question: For each Water-to-Water Heat Pump, what are the related non-writable property labels and ALC and MPC endpoint references?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: LBNL_B59_005 ---\n",
      "Description: Retrieves volume or heat flow rate properties for each Underfloor Fan Terminal (UFT).\n",
      "\n",
      "  - Question: What volume or heat flow rate properties does each UFT have, including their labels, aspects, values, and units?\n",
      "    Source: human\n",
      "\n",
      "  - Question: What is the label of each UFT?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: LBNL_B59_006 ---\n",
      "Description: Identifies the labels of UFTs, the zones they are connected to, and the AHUs that supply them.\n",
      "\n",
      "  - Question: For each UFT, what is its label, the label of the zone it's indirectly connected to (through a junction and domain space), and the label of the AHU that indirectly supplies it?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: LBNL_B59_007 ---\n",
      "Description: Identifies air temperature sensors in HVAC core spaces, retrieving their labels and the properties they observe.\n",
      "\n",
      "  - Question: What are the labels of the air temperature sensors (in Â°F), their quantifiable observable properties, and the domain spaces theyâ€™re located in, specifically for sensors placed in HVAC core spaces?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: TUC_001 ---\n",
      "Description: Retrieves the timeseries ID of the maximum air temperature setpoint for each zone.\n",
      "\n",
      "  - Question: For each zone, what is the timeseries ID of its maximum air temperature setpoint, and what is the zoneâ€™s IFC reference?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: TUC_002 ---\n",
      "Description: Retrieves the timeseries ID of the minimum air temperature setpoint for each zone.\n",
      "\n",
      "  - Question: For each zone, what is the timeseries ID of its minimum air temperature setpoint, and what is the zoneâ€™s IFC reference?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: TUC_003 ---\n",
      "Description: Retrieves the timeseries ID of the occupancy sensor for each zone.\n",
      "\n",
      "  - Question: For each zone, what is the timeseries ID of its occupancy sensor, and what is the zoneâ€™s IFC reference?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: TUC_004 ---\n",
      "Description: Retrieves the timeseries ID of the temperature setpoint for each zone.\n",
      "\n",
      "  - Question: For each zone, what is the timeseries ID of its temperature setpoint, and what is the zoneâ€™s IFC reference?\n",
      "    Source: human\n",
      "\n",
      "--- Query ID: TUC_005 ---\n",
      "Description: Retrieves the timeseries ID of the temperature sensor for each zone.\n",
      "\n",
      "  - Question: For each zone, what is the timeseries ID of its temperature sensor, and what is the zoneâ€™s IFC reference?\n",
      "    Source: human\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# The name of your JSON file\n",
    "file_name = 'flavia.json'\n",
    "\n",
    "try:\n",
    "    # Open and load the JSON file\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Loop through each query object in the list\n",
    "    for query_obj in data:\n",
    "        query_id = query_obj.get('query_id', 'N/A')\n",
    "        description = query_obj.get('description', 'No description')\n",
    "        \n",
    "        print(f\"--- Query ID: {query_id} ---\")\n",
    "        print(f\"Description: {description}\\n\")\n",
    "\n",
    "        # Loop through the questions for the current query\n",
    "        for question in query_obj.get('questions', []):\n",
    "            question_text = question.get('text', 'No question text')\n",
    "            question_source = question.get('source', 'N/A')\n",
    "            print(f\"  - Question: {question_text}\")\n",
    "            print(f\"    Source: {question_source}\\n\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_name}' was not found.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: The file '{file_name}' is not a valid JSON file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c63c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph created.\n",
      "Parsing Brick.ttl...\n",
      "Successfully parsed Brick.ttl.\n",
      "Skolemized and merged Brick.ttl.\n",
      "Parsing brick-example.ttl...\n",
      "Successfully parsed brick-example.ttl.\n",
      "Skolemized and merged brick-example.ttl.\n",
      "\n",
      "Processing complete. The graph contains 57389 triples.\n",
      "\n",
      "Graph 'brick_parsed' has been created in memory.\n",
      "\n",
      "Graph has been serialized and saved to brick_parsed.ttl.\n"
     ]
    }
   ],
   "source": [
    "# requirements: pip install rdflib\n",
    "\n",
    "import rdflib\n",
    "\n",
    "# requirements: pip install rdflib\n",
    "\n",
    "import rdflib\n",
    "from rdflib.namespace import RDFS\n",
    "\n",
    "def parse_and_skolemize_ttl_files(file_paths):\n",
    "    \"\"\"\n",
    "    Parses one or more TTL files into a single RDFLib Graph, replacing\n",
    "    blank nodes (BNodes) with stable URIs (skolemization).\n",
    "\n",
    "    Args:\n",
    "        file_paths (list): A list of strings, where each string is the\n",
    "                           path to a TTL file.\n",
    "\n",
    "    Returns:\n",
    "        rdflib.Graph: A graph object containing the parsed and skolemized\n",
    "                      data from all files, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    # 1. Create a new Graph object. This will hold the combined data.\n",
    "    g = rdflib.Graph()\n",
    "    print(\"Graph created.\")\n",
    "\n",
    "    # 2. Iterate through the list of file paths provided.\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            # 3. Parse each file into a temporary graph.\n",
    "            print(f\"Parsing {file_path}...\")\n",
    "            temp_g = rdflib.Graph()\n",
    "            temp_g.parse(file_path, format='turtle')\n",
    "            print(f\"Successfully parsed {file_path}.\")\n",
    "\n",
    "            # 4. Skolemize the temporary graph to replace blank nodes.\n",
    "            # This replaces BNodes with unique and valid URIs.\n",
    "            # The base URI for the new nodes will be 'http://example.org/.well-known/genid/'\n",
    "            temp_g.skolemize()\n",
    "            \n",
    "            # 5. Add the triples from the skolemized temp graph to the main graph.\n",
    "            g += temp_g\n",
    "            print(f\"Skolemized and merged {file_path}.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file at {file_path} was not found.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # 6. The graph `g` now contains all the triples from your files without BNodes.\n",
    "    print(f\"\\nProcessing complete. The graph contains {len(g)} triples.\")\n",
    "    return g\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    # List of the TTL files you want to parse.\n",
    "    files_to_parse = ['Brick.ttl', 'brick-example.ttl']#['223p.ttl', 's223-example.ttl']\n",
    "#['Brick (1).ttl', 'brick-example.ttl']#\n",
    "    # Call the function to parse the files.\n",
    "    brick_parsed = parse_and_skolemize_ttl_files(files_to_parse)\n",
    "\n",
    "    if brick_parsed:\n",
    "        print(\"\\nGraph 'brick_parsed' has been created in memory.\")\n",
    "\n",
    "    #save the graph as a ttl file\n",
    "    output_file = 'brick_parsed.ttl'\n",
    "    try:\n",
    "        brick_parsed.serialize(destination=output_file, format='turtle')\n",
    "        print(f\"\\nGraph has been serialized and saved to {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the graph: {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f478ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading RDF data from 'dflexlibs_multizone.ttl' ---\n",
      "âœ… RDF data loaded successfully.\n",
      "\n",
      "--- Loading queries from 'dflexlibs_multizone.json' ---\n",
      "\n",
      "ğŸ¢ Processing 6 queries for building: multizone_office_simple_air_with_data.ttl\n",
      "\n",
      "â–¶ï¸ Running DFLEXLIBS_001: BOPTEST building testcase_multizone_office_simple_air (Commercial building - 8500 m2)\n",
      "   ğŸ” Found 1080 result(s)\n",
      "       zone_name        zone_temp_point set_temp_min_point set_temp_max_point  \\\n",
      "0  hvac_cor_zone  hvac_reaZonCor_TZon_y     LowerSetp[cor]     UpperSetp[cor]   \n",
      "1  hvac_cor_zone  hvac_reaZonCor_TZon_y     LowerSetp[cor]     UpperSetp[cor]   \n",
      "2  hvac_cor_zone  hvac_reaZonCor_TZon_y     LowerSetp[cor]     UpperSetp[cor]   \n",
      "3  hvac_cor_zone  hvac_reaZonCor_TZon_y     LowerSetp[cor]     UpperSetp[cor]   \n",
      "4  hvac_cor_zone  hvac_reaZonCor_TZon_y     LowerSetp[cor]     UpperSetp[cor]   \n",
      "\n",
      "  occ_sensor_point occ_cmd_point        zone_set_temp_heat_point  \\\n",
      "0   Occupancy[cor]          None  hvac_oveZonSupCor_TZonHeaSet_u   \n",
      "1   Occupancy[cor]          None  hvac_oveZonSupCor_TZonHeaSet_u   \n",
      "2   Occupancy[cor]          None  hvac_oveZonSupCor_TZonHeaSet_u   \n",
      "3   Occupancy[cor]          None  hvac_oveZonSupCor_TZonHeaSet_u   \n",
      "4   Occupancy[cor]          None  hvac_oveZonSupCor_TZonHeaSet_u   \n",
      "\n",
      "         zone_set_temp_cool_point unocc_zone_set_temp_heat_point  \\\n",
      "0  hvac_oveZonSupCor_TZonCooSet_u                           None   \n",
      "1  hvac_oveZonSupCor_TZonCooSet_u                           None   \n",
      "2  hvac_oveZonSupCor_TZonCooSet_u                           None   \n",
      "3  hvac_oveZonSupCor_TZonCooSet_u                           None   \n",
      "4  hvac_oveZonSupCor_TZonCooSet_u                           None   \n",
      "\n",
      "  unocc_zone_set_temp_cool_point occ_zone_set_temp_heat_point  \\\n",
      "0                           None                         None   \n",
      "1                           None                         None   \n",
      "2                           None                         None   \n",
      "3                           None                         None   \n",
      "4                           None                         None   \n",
      "\n",
      "  occ_zone_set_temp_cool_point zone_set_temp_point      vav_damper_set_point  \\\n",
      "0                         None                None  hvac_oveZonActCor_yDam_u   \n",
      "1                         None                None  hvac_oveZonActCor_yDam_u   \n",
      "2                         None                None  hvac_oveZonActCor_yDam_u   \n",
      "3                         None                None  hvac_oveZonActCor_yDam_u   \n",
      "4                         None                None  hvac_oveZonActCor_yDam_u   \n",
      "\n",
      "  vav_discharge_temp_point     vav_reheat_command_point  \n",
      "0    hvac_reaZonCor_TSup_y  hvac_oveZonActCor_yReaHea_u  \n",
      "1    hvac_reaZonCor_TSup_y  hvac_oveZonActCor_yReaHea_u  \n",
      "2    hvac_reaZonCor_TSup_y  hvac_oveZonActCor_yReaHea_u  \n",
      "3    hvac_reaZonCor_TSup_y  hvac_oveZonActCor_yReaHea_u  \n",
      "4    hvac_reaZonCor_TSup_y  hvac_oveZonActCor_yReaHea_u  \n",
      "â–¶ï¸ Running DFLEXLIBS_002: BOPTEST building testcase_multizone_office_simple_air (Commercial building - 8500 m2)\n",
      "   ğŸ” Found 5 result(s)\n",
      "  ahu_supply_temp_point     ahu_supply_flow_point ahu_supply_flow_set_point\n",
      "0    hvac_reaAhu_TSup_y  hvac_reaAhu_V_flow_sup_y                      None\n",
      "1    hvac_reaAhu_TSup_y  hvac_reaAhu_V_flow_sup_y                      None\n",
      "2    hvac_reaAhu_TSup_y  hvac_reaAhu_V_flow_sup_y                      None\n",
      "3    hvac_reaAhu_TSup_y  hvac_reaAhu_V_flow_sup_y                      None\n",
      "4    hvac_reaAhu_TSup_y  hvac_reaAhu_V_flow_sup_y                      None\n",
      "â–¶ï¸ Running DFLEXLIBS_003: BOPTEST building testcase_multizone_office_simple_air (Commercial building - 8500 m2)\n",
      "   ğŸ” Found 7 result(s)\n",
      "           ele_pow_point therm_pow_point\n",
      "0          chi_reaPChi_y            None\n",
      "1       chi_reaPPumDis_y            None\n",
      "2    heaPum_reaPHeaPum_y            None\n",
      "3    heaPum_reaPPumDis_y            None\n",
      "4  hvac_reaAhu_PFanSup_y            None\n",
      "â–¶ï¸ Running DFLEXLIBS_004: BOPTEST building testcase_multizone_office_simple_air (Commercial building - 8500 m2)\n",
      "   ğŸ” Found 1 result(s)\n",
      "      area\n",
      "0  1662.66\n",
      "â–¶ï¸ Running DFLEXLIBS_005: BOPTEST building testcase_multizone_office_simple_air (Commercial building - 8500 m2)\n",
      "   ğŸ” Found 7 result(s)\n",
      "             power_point\n",
      "0          chi_reaPChi_y\n",
      "1       chi_reaPPumDis_y\n",
      "2    heaPum_reaPHeaPum_y\n",
      "3    heaPum_reaPPumDis_y\n",
      "4  hvac_reaAhu_PFanSup_y\n",
      "â–¶ï¸ Running DFLEXLIBS_006: BOPTEST building testcase_multizone_office_simple_air (Commercial building - 8500 m2)\n",
      "   ğŸ” Found 7 result(s)\n",
      "             power_point\n",
      "0          chi_reaPChi_y\n",
      "1       chi_reaPPumDis_y\n",
      "2    heaPum_reaPHeaPum_y\n",
      "3    heaPum_reaPPumDis_y\n",
      "4  hvac_reaAhu_PFanSup_y\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# --- Helper Function ---\n",
    "def extract_query_result_as_dataframe(sparql_result):\n",
    "    var_names = [str(var) for var in sparql_result.vars]\n",
    "    result_list = list(sparql_result)\n",
    "    df = pd.DataFrame(result_list, columns=var_names)\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].apply(\n",
    "            lambda x: x.split(\"#\")[-1] if isinstance(x, rdflib.term.URIRef) else str(x) if x is not None else None\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# --- Configuration ---\n",
    "ttl_file = \"dflexlibs_multizone.ttl\"\n",
    "json_file = \"dflexlibs_multizone.json\"\n",
    "\n",
    "# --- Load RDF Graph ---\n",
    "print(f\"\\n--- Loading RDF data from '{ttl_file}' ---\")\n",
    "g = rdflib.Graph()\n",
    "try:\n",
    "    g.parse(ttl_file, format=\"turtle\")\n",
    "    print(\"âœ… RDF data loaded successfully.\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to load RDF file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Load Query JSON ---\n",
    "print(f\"--- Loading queries from '{json_file}' ---\")\n",
    "try:\n",
    "    with open(json_file, 'r') as f:\n",
    "        all_building_data = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to load JSON file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Run Queries ---\n",
    "for bldg_obj in all_building_data:\n",
    "    queries = bldg_obj.get(\"queries\", [])\n",
    "    print(f\"\\nğŸ¢ Processing {len(queries)} queries for building: {bldg_obj.get('building_id', 'unknown')}\\n\")\n",
    "    \n",
    "    for i, query_obj in enumerate(queries):\n",
    "        query_id = query_obj.get(\"query_id\", f\"Query_{i}\")\n",
    "        sparql_query = query_obj.get(\"sparql_query\")\n",
    "\n",
    "        if not sparql_query:\n",
    "            print(f\"âš ï¸ Skipping {query_id}: No SPARQL query found.\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"â–¶ï¸ Running {query_id}: {query_obj.get('description', '')}\")\n",
    "        try:\n",
    "            result = g.query(sparql_query)\n",
    "            result_list = list(result)\n",
    "            print(f\"   ğŸ” Found {len(result_list)} result(s)\")\n",
    "\n",
    "            if result_list:\n",
    "                df = extract_query_result_as_dataframe(result)\n",
    "                print(df.head())\n",
    "            else:\n",
    "                print(\"   âš ï¸ No results returned.\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error running query {query_id}: {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc3453d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created or cleared output directory: 'mortar_updated2'\n",
      "\n",
      "--- Starting File Content Replacement ---\n",
      "Processing 'mortargraphs_updated/bldg5.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg5.ttl'\n",
      "Processing 'mortargraphs_updated/bldg4.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg4.ttl'\n",
      "Processing 'mortargraphs_updated/bldg6.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg6.ttl'\n",
      "Processing 'mortargraphs_updated/bldg44.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg44.ttl'\n",
      "Processing 'mortargraphs_updated/bldg7.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg7.ttl'\n",
      "Processing 'mortargraphs_updated/bldg3.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg3.ttl'\n",
      "Processing 'mortargraphs_updated/bldg41.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg41.ttl'\n",
      "Processing 'mortargraphs_updated/bldg40.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg40.ttl'\n",
      "Processing 'mortargraphs_updated/bldg2.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg2.ttl'\n",
      "Processing 'mortargraphs_updated/bldg42.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg42.ttl'\n",
      "Processing 'mortargraphs_updated/bldg43.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg43.ttl'\n",
      "Processing 'mortargraphs_updated/bldg1.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg1.ttl'\n",
      "Processing 'mortargraphs_updated/bldg18.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg18.ttl'\n",
      "Processing 'mortargraphs_updated/bldg24.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg24.ttl'\n",
      "Processing 'mortargraphs_updated/bldg31.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg31.ttl'\n",
      "Processing 'mortargraphs_updated/bldg25.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg25.ttl'\n",
      "Processing 'mortargraphs_updated/bldg19.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg19.ttl'\n",
      "Processing 'mortargraphs_updated/bldg33.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg33.ttl'\n",
      "Processing 'mortargraphs_updated/bldg27.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg27.ttl'\n",
      "Processing 'mortargraphs_updated/bldg26.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg26.ttl'\n",
      "Processing 'mortargraphs_updated/bldg32.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg32.ttl'\n",
      "Processing 'mortargraphs_updated/smc.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/smc.ttl'\n",
      "Processing 'mortargraphs_updated/bldg36.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg36.ttl'\n",
      "Processing 'mortargraphs_updated/bldg22.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg22.ttl'\n",
      "Processing 'mortargraphs_updated/bldg23.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg23.ttl'\n",
      "Processing 'mortargraphs_updated/bldg37.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg37.ttl'\n",
      "Processing 'mortargraphs_updated/bldg21.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg21.ttl'\n",
      "Processing 'mortargraphs_updated/bldg35.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg35.ttl'\n",
      "Processing 'mortargraphs_updated/bldg34.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg34.ttl'\n",
      "Processing 'mortargraphs_updated/bldg20.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg20.ttl'\n",
      "Processing 'mortargraphs_updated/bldg39.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg39.ttl'\n",
      "Processing 'mortargraphs_updated/bldg11.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg11.ttl'\n",
      "Processing 'mortargraphs_updated/bldg10.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg10.ttl'\n",
      "Processing 'mortargraphs_updated/bldg38.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg38.ttl'\n",
      "Processing 'mortargraphs_updated/bldg12.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg12.ttl'\n",
      "Processing 'mortargraphs_updated/bldg13.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg13.ttl'\n",
      "Processing 'mortargraphs_updated/bldg17.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg17.ttl'\n",
      "Processing 'mortargraphs_updated/bldg16.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg16.ttl'\n",
      "Processing 'mortargraphs_updated/bldg14.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg14.ttl'\n",
      "Processing 'mortargraphs_updated/bldg28.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg28.ttl'\n",
      "Processing 'mortargraphs_updated/bldg29.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg29.ttl'\n",
      "Processing 'mortargraphs_updated/bldg15.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg15.ttl'\n",
      "Processing 'mortargraphs_updated/bldg9.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg9.ttl'\n",
      "Processing 'mortargraphs_updated/bldg8.ttl'...\n",
      " -> Saved updated file to 'mortar_updated2/bldg8.ttl'\n",
      "\n",
      "Finished processing. A total of 44 TTL files were updated.\n",
      "\n",
      "--- Starting SPARQL Query Test on Updated Files ---\n",
      "Testing 'bldg5.ttl'...\n",
      "  --> SUCCESS: Found 12 results in 'bldg5.ttl'.\n",
      "Testing 'bldg4.ttl'...\n",
      "  --> SUCCESS: Found 2 results in 'bldg4.ttl'.\n",
      "Testing 'bldg6.ttl'...\n",
      "  --> SUCCESS: Found 12 results in 'bldg6.ttl'.\n",
      "Testing 'bldg44.ttl'...\n",
      "  --> SUCCESS: Found 6 results in 'bldg44.ttl'.\n",
      "Testing 'bldg7.ttl'...\n",
      "  --> SUCCESS: Found 9 results in 'bldg7.ttl'.\n",
      "Testing 'bldg3.ttl'...\n",
      "  --> SUCCESS: Found 1 results in 'bldg3.ttl'.\n",
      "Testing 'bldg41.ttl'...\n",
      "  --> SUCCESS: Found 4 results in 'bldg41.ttl'.\n",
      "Testing 'bldg40.ttl'...\n",
      "  --> SUCCESS: Found 4 results in 'bldg40.ttl'.\n",
      "Testing 'bldg2.ttl'...\n",
      "  --> SUCCESS: Found 3 results in 'bldg2.ttl'.\n",
      "Testing 'bldg42.ttl'...\n",
      "  --> SUCCESS: Found 10 results in 'bldg42.ttl'.\n",
      "Testing 'bldg43.ttl'...\n",
      "  --> No results found in 'bldg43.ttl'.\n",
      "Testing 'bldg1.ttl'...\n",
      "  --> No results found in 'bldg1.ttl'.\n",
      "Testing 'bldg18.ttl'...\n",
      "  --> SUCCESS: Found 10 results in 'bldg18.ttl'.\n",
      "Testing 'bldg24.ttl'...\n",
      "  --> SUCCESS: Found 1 results in 'bldg24.ttl'.\n",
      "Testing 'bldg31.ttl'...\n",
      "  --> No results found in 'bldg31.ttl'.\n",
      "Testing 'bldg25.ttl'...\n",
      "  --> SUCCESS: Found 2 results in 'bldg25.ttl'.\n",
      "Testing 'bldg19.ttl'...\n",
      "  --> SUCCESS: Found 10 results in 'bldg19.ttl'.\n",
      "Testing 'bldg33.ttl'...\n",
      "  --> No results found in 'bldg33.ttl'.\n",
      "Testing 'bldg27.ttl'...\n",
      "  --> No results found in 'bldg27.ttl'.\n",
      "Testing 'bldg26.ttl'...\n",
      "  --> SUCCESS: Found 3 results in 'bldg26.ttl'.\n",
      "Testing 'bldg32.ttl'...\n",
      "  --> SUCCESS: Found 7 results in 'bldg32.ttl'.\n",
      "Testing 'smc.ttl'...\n",
      "  --> No results found in 'smc.ttl'.\n",
      "Testing 'bldg36.ttl'...\n",
      "  --> No results found in 'bldg36.ttl'.\n",
      "Testing 'bldg22.ttl'...\n",
      "  --> SUCCESS: Found 8 results in 'bldg22.ttl'.\n",
      "Testing 'bldg23.ttl'...\n",
      "  --> SUCCESS: Found 3 results in 'bldg23.ttl'.\n",
      "Testing 'bldg37.ttl'...\n",
      "  --> SUCCESS: Found 13 results in 'bldg37.ttl'.\n",
      "Testing 'bldg21.ttl'...\n",
      "  --> SUCCESS: Found 6 results in 'bldg21.ttl'.\n",
      "Testing 'bldg35.ttl'...\n",
      "  --> SUCCESS: Found 32 results in 'bldg35.ttl'.\n",
      "Testing 'bldg34.ttl'...\n",
      "  --> No results found in 'bldg34.ttl'.\n",
      "Testing 'bldg20.ttl'...\n",
      "  --> SUCCESS: Found 2 results in 'bldg20.ttl'.\n",
      "Testing 'bldg39.ttl'...\n",
      "  --> SUCCESS: Found 1 results in 'bldg39.ttl'.\n",
      "Testing 'bldg11.ttl'...\n",
      "  --> SUCCESS: Found 19 results in 'bldg11.ttl'.\n",
      "Testing 'bldg10.ttl'...\n",
      "  --> SUCCESS: Found 6 results in 'bldg10.ttl'.\n",
      "Testing 'bldg38.ttl'...\n",
      "  --> No results found in 'bldg38.ttl'.\n",
      "Testing 'bldg12.ttl'...\n",
      "  --> SUCCESS: Found 4 results in 'bldg12.ttl'.\n",
      "Testing 'bldg13.ttl'...\n",
      "  --> SUCCESS: Found 2 results in 'bldg13.ttl'.\n",
      "Testing 'bldg17.ttl'...\n",
      "  --> No results found in 'bldg17.ttl'.\n",
      "Testing 'bldg16.ttl'...\n",
      "  --> No results found in 'bldg16.ttl'.\n",
      "Testing 'bldg14.ttl'...\n",
      "  --> No results found in 'bldg14.ttl'.\n",
      "Testing 'bldg28.ttl'...\n",
      "  --> SUCCESS: Found 2 results in 'bldg28.ttl'.\n",
      "Testing 'bldg29.ttl'...\n",
      "  --> SUCCESS: Found 2 results in 'bldg29.ttl'.\n",
      "Testing 'bldg15.ttl'...\n",
      "  --> No results found in 'bldg15.ttl'.\n",
      "Testing 'bldg9.ttl'...\n",
      "  --> No results found in 'bldg9.ttl'.\n",
      "Testing 'bldg8.ttl'...\n",
      "  --> SUCCESS: Found 2 results in 'bldg8.ttl'.\n",
      "\n",
      "--- Test Summary ---\n",
      "The following buildings returned a result for the query:\n",
      "- bldg5.ttl\n",
      "- bldg4.ttl\n",
      "- bldg6.ttl\n",
      "- bldg44.ttl\n",
      "- bldg7.ttl\n",
      "- bldg3.ttl\n",
      "- bldg41.ttl\n",
      "- bldg40.ttl\n",
      "- bldg2.ttl\n",
      "- bldg42.ttl\n",
      "- bldg18.ttl\n",
      "- bldg24.ttl\n",
      "- bldg25.ttl\n",
      "- bldg19.ttl\n",
      "- bldg26.ttl\n",
      "- bldg32.ttl\n",
      "- bldg22.ttl\n",
      "- bldg23.ttl\n",
      "- bldg37.ttl\n",
      "- bldg21.ttl\n",
      "- bldg35.ttl\n",
      "- bldg20.ttl\n",
      "- bldg39.ttl\n",
      "- bldg11.ttl\n",
      "- bldg10.ttl\n",
      "- bldg12.ttl\n",
      "- bldg13.ttl\n",
      "- bldg28.ttl\n",
      "- bldg29.ttl\n",
      "- bldg8.ttl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import rdflib\n",
    "\n",
    "def update_ttl_files(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Reads all TTL files from an input directory, replaces a specific string,\n",
    "    and saves them to an output directory.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): The path to the directory containing the original\n",
    "                         TTL files.\n",
    "        output_dir (str): The path to the directory where the updated TTL\n",
    "                          files will be saved.\n",
    "    \"\"\"\n",
    "    # --- Create Output Directory ---\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f\"Output directory '{output_dir}' already exists. Clearing it before proceeding.\")\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Successfully created or cleared output directory: '{output_dir}'\")\n",
    "\n",
    "    # --- Process and Replace String in Files ---\n",
    "    try:\n",
    "        files_processed = 0\n",
    "        print(\"\\n--- Starting File Content Replacement ---\")\n",
    "        for filename in os.listdir(input_dir):\n",
    "            if filename.endswith(\".ttl\"):\n",
    "                input_file_path = os.path.join(input_dir, filename)\n",
    "                output_file_path = os.path.join(output_dir, filename)\n",
    "\n",
    "                print(f\"Processing '{input_file_path}'...\")\n",
    "                try:\n",
    "                    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    updated_content = content.replace(\"Air_Handler_Unit\", \"Air_Handling_Unit\")\n",
    "\n",
    "                    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(updated_content)\n",
    "                    \n",
    "                    files_processed += 1\n",
    "                    print(f\" -> Saved updated file to '{output_file_path}'\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {filename}: {e}\")\n",
    "        \n",
    "        if files_processed == 0:\n",
    "            print(f\"\\nNo '.ttl' files were found in '{input_dir}'.\")\n",
    "        else:\n",
    "            print(f\"\\nFinished processing. A total of {files_processed} TTL files were updated.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The input directory '{input_dir}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def test_updated_buildings(directory, sparql_query):\n",
    "    \"\"\"\n",
    "    Loads each TTL file from a directory and runs a SPARQL query,\n",
    "    reporting which files return results.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory containing the TTL files to test.\n",
    "        sparql_query (str): The SPARQL query to execute.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting SPARQL Query Test on Updated Files ---\")\n",
    "    buildings_with_results = []\n",
    "    \n",
    "    try:\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".ttl\"):\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                print(f\"Testing '{filename}'...\")\n",
    "\n",
    "                try:\n",
    "                    g = rdflib.Graph()\n",
    "                    g.parse(file_path, format='turtle')\n",
    "                    \n",
    "                    results = g.query(sparql_query)\n",
    "                    \n",
    "                    if len(results) > 0:\n",
    "                        print(f\"  --> SUCCESS: Found {len(results)} results in '{filename}'.\")\n",
    "                        buildings_with_results.append(filename)\n",
    "                    else:\n",
    "                        print(f\"  --> No results found in '{filename}'.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  --> ERROR: Could not parse or query '{filename}'. Reason: {e}\")\n",
    "        \n",
    "        print(\"\\n--- Test Summary ---\")\n",
    "        if buildings_with_results:\n",
    "            print(\"The following buildings returned a result for the query:\")\n",
    "            for building in buildings_with_results:\n",
    "                print(f\"- {building}\")\n",
    "        else:\n",
    "            print(\"No buildings returned a result for the specified SPARQL query.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The directory '{directory}' was not found for testing.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during testing: {e}\")\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "input_directory = \"mortargraphs_updated\"\n",
    "output_directory = \"mortar_updated2\"\n",
    "\n",
    "SPARQL_QUERY_TO_TEST = \"\"\"\n",
    "    SELECT ?cooling_point ?heating_point ?ahu WHERE {\n",
    "        ?ahu rdf:type/rdfs:subClassOf* brick:Air_Handling_Unit .\n",
    "        ?cooling_point rdf:type/rdfs:subClassOf* brick:Cooling_Command .\n",
    "        ?heating_point rdf:type/rdfs:subClassOf* brick:Heating_Command .\n",
    "        ?ahu brick:hasPoint ?cooling_point .\n",
    "        ?ahu brick:hasPoint ?heating_point .\n",
    "    }\n",
    "\"\"\"\n",
    "# --- End Configuration ---\n",
    "\n",
    "# --- Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.isdir(input_directory):\n",
    "         print(f\"Error: Input directory '{input_directory}' not found.\")\n",
    "         print(\"Please create it and place your TTL files inside.\")\n",
    "    else:\n",
    "        # Step 1: Update the TTL files\n",
    "        update_ttl_files(input_directory, output_directory)\n",
    "        \n",
    "        # Step 2: Test the newly created files with the SPARQL query\n",
    "        test_updated_buildings(output_directory, SPARQL_QUERY_TO_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec1b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing Triple Counts in 'mortargraphs_updated' ---\n",
      "Found 44 TTL files to process...\n",
      "  - Processing 'bldg5.ttl'...\n",
      "  - Processing 'bldg4.ttl'...\n",
      "  - Processing 'bldg6.ttl'...\n",
      "  - Processing 'bldg44.ttl'...\n",
      "  - Processing 'bldg7.ttl'...\n",
      "  - Processing 'bldg3.ttl'...\n",
      "  - Processing 'bldg41.ttl'...\n",
      "  - Processing 'bldg40.ttl'...\n",
      "  - Processing 'bldg2.ttl'...\n",
      "  - Processing 'bldg42.ttl'...\n",
      "  - Processing 'bldg43.ttl'...\n",
      "  - Processing 'bldg1.ttl'...\n",
      "  - Processing 'bldg18.ttl'...\n",
      "  - Processing 'bldg24.ttl'...\n",
      "  - Processing 'bldg31.ttl'...\n",
      "  - Processing 'bldg25.ttl'...\n",
      "  - Processing 'bldg19.ttl'...\n",
      "  - Processing 'bldg33.ttl'...\n",
      "  - Processing 'bldg27.ttl'...\n",
      "  - Processing 'bldg26.ttl'...\n",
      "  - Processing 'bldg32.ttl'...\n",
      "  - Processing 'smc.ttl'...\n",
      "  - Processing 'bldg36.ttl'...\n",
      "  - Processing 'bldg22.ttl'...\n",
      "  - Processing 'bldg23.ttl'...\n",
      "  - Processing 'bldg37.ttl'...\n",
      "  - Processing 'bldg21.ttl'...\n",
      "  - Processing 'bldg35.ttl'...\n",
      "  - Processing 'bldg34.ttl'...\n",
      "  - Processing 'bldg20.ttl'...\n",
      "  - Processing 'bldg39.ttl'...\n",
      "  - Processing 'bldg11.ttl'...\n",
      "  - Processing 'bldg10.ttl'...\n",
      "  - Processing 'bldg38.ttl'...\n",
      "  - Processing 'bldg12.ttl'...\n",
      "  - Processing 'bldg13.ttl'...\n",
      "  - Processing 'bldg17.ttl'...\n",
      "  - Processing 'bldg16.ttl'...\n",
      "  - Processing 'bldg14.ttl'...\n",
      "  - Processing 'bldg28.ttl'...\n",
      "  - Processing 'bldg29.ttl'...\n",
      "  - Processing 'bldg15.ttl'...\n",
      "  - Processing 'bldg9.ttl'...\n",
      "  - Processing 'bldg8.ttl'...\n",
      "\n",
      "--- Analysis Results ---\n",
      "Minimum Triples: 17 (File(s): bldg24.ttl)\n",
      "Median Triples:  566 (Closest File(s): bldg2.ttl, bldg1.ttl)\n",
      "Maximum Triples: 10972 (File(s): bldg37.ttl)\n",
      "\n",
      "--- Full Data (sorted by triple count) ---\n",
      "      filename  triples\n",
      "0   bldg24.ttl       17\n",
      "1   bldg33.ttl       26\n",
      "2   bldg31.ttl       31\n",
      "3   bldg17.ttl       56\n",
      "4   bldg38.ttl       59\n",
      "5    bldg3.ttl       62\n",
      "6   bldg14.ttl       62\n",
      "7   bldg29.ttl       76\n",
      "8   bldg16.ttl       90\n",
      "9   bldg25.ttl      114\n",
      "10  bldg41.ttl      125\n",
      "11  bldg23.ttl      167\n",
      "12  bldg26.ttl      213\n",
      "13  bldg21.ttl      244\n",
      "14  bldg22.ttl      291\n",
      "15  bldg19.ttl      309\n",
      "16   bldg7.ttl      439\n",
      "17  bldg39.ttl      448\n",
      "18  bldg10.ttl      448\n",
      "19  bldg35.ttl      466\n",
      "20  bldg20.ttl      494\n",
      "21   bldg1.ttl      529\n",
      "22   bldg2.ttl      604\n",
      "23  bldg42.ttl      639\n",
      "24  bldg27.ttl      645\n",
      "25  bldg28.ttl      766\n",
      "26  bldg13.ttl      825\n",
      "27  bldg36.ttl      829\n",
      "28   bldg4.ttl      929\n",
      "29  bldg44.ttl      936\n",
      "30   bldg8.ttl     1001\n",
      "31  bldg43.ttl     1272\n",
      "32  bldg34.ttl     1505\n",
      "33  bldg40.ttl     2176\n",
      "34  bldg12.ttl     2624\n",
      "35     smc.ttl     2924\n",
      "36   bldg9.ttl     3427\n",
      "37   bldg5.ttl     3456\n",
      "38  bldg18.ttl     3469\n",
      "39   bldg6.ttl     4093\n",
      "40  bldg32.ttl     4370\n",
      "41  bldg15.ttl     5397\n",
      "42  bldg11.ttl     8618\n",
      "43  bldg37.ttl    10972\n"
     ]
    }
   ],
   "source": [
    "# You must have the required libraries installed:\n",
    "# pip install rdflib pandas numpy\n",
    "\n",
    "import os\n",
    "import rdflib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "graphs_directory = \"mortargraphs_updated\"\n",
    "# --- End Configuration ---\n",
    "\n",
    "file_triple_counts = []\n",
    "\n",
    "print(f\"--- Analyzing Triple Counts in '{graphs_directory}' ---\")\n",
    "\n",
    "# Get a list of all files in the specified directory and filter for .ttl files\n",
    "all_files = os.listdir(graphs_directory)\n",
    "ttl_files = [f for f in all_files if f.endswith('.ttl')]\n",
    "\n",
    "print(f\"Found {len(ttl_files)} TTL files to process...\")\n",
    "\n",
    "# Loop through each .ttl file\n",
    "for ttl_filename in ttl_files:\n",
    "    file_path = os.path.join(graphs_directory, ttl_filename)\n",
    "    print(f\"  - Processing '{ttl_filename}'...\")\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(file_path, format='turtle')\n",
    "    num_triples = len(g)\n",
    "    file_triple_counts.append({'filename': ttl_filename, 'triples': num_triples})\n",
    "\n",
    "# After processing all files, analyze the results\n",
    "# Convert the list of dictionaries to a pandas DataFrame for easy analysis\n",
    "counts_df = pd.DataFrame(file_triple_counts)\n",
    "\n",
    "# Calculate min, median, and max values\n",
    "min_triples = counts_df['triples'].min()\n",
    "median_triples = counts_df['triples'].median()\n",
    "max_triples = counts_df['triples'].max()\n",
    "\n",
    "# Find the corresponding filenames\n",
    "min_files = counts_df[counts_df['triples'] == min_triples]['filename'].tolist()\n",
    "max_files = counts_df[counts_df['triples'] == max_triples]['filename'].tolist()\n",
    "\n",
    "# For the median, find the file(s) with the triple count closest to the median value\n",
    "counts_df['median_diff'] = abs(counts_df['triples'] - median_triples)\n",
    "min_median_diff = counts_df['median_diff'].min()\n",
    "median_files = counts_df[counts_df['median_diff'] == min_median_diff]['filename'].tolist()\n",
    "\n",
    "print(\"\\n--- Analysis Results ---\")\n",
    "print(f\"Minimum Triples: {min_triples} (File(s): {', '.join(min_files)})\")\n",
    "print(f\"Median Triples:  {median_triples:.0f} (Closest File(s): {', '.join(median_files)})\")\n",
    "print(f\"Maximum Triples: {max_triples} (File(s): {', '.join(max_files)})\")\n",
    "\n",
    "print(\"\\n--- Full Data (sorted by triple count) ---\")\n",
    "# Set pandas to display all rows for better viewing\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(counts_df[['filename', 'triples']].sort_values(by='triples').reset_index(drop=True))\n",
    "# Reset the display option if desired\n",
    "pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b34e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Analyzing Triple Counts in 'mortar_bricked' ---\n",
      "Analyzed 44 building graphs.\n",
      "\n",
      "--- Step 2: Loading queries from 'mortar.json' ---\n",
      "Loaded 9 queries.\n",
      "\n",
      "--- Step 3: Finding the most successful buildings ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     results \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mquery(sparql_query)\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# If the query returns results, increment the counter for that building\u001b[39;00m\n\u001b[1;32m     54\u001b[0m         building_success_counts[ttl_filename] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# In case a SPARQL query has a syntax error\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.9/site-packages/rdflib/query.py:347\u001b[0m, in \u001b[0;36mResult.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbindings\u001b[49m)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# type error: Argument 1 to \"len\" has incompatible type \"Optional[Graph]\"; expected \"Sized\"\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph)\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.9/site-packages/rdflib/query.py:242\u001b[0m, in \u001b[0;36mResult.bindings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03ma list of variable bindings as dicts\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_genbindings:\n\u001b[0;32m--> 242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bindings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_genbindings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_genbindings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bindings\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.9/site-packages/rdflib/plugins/sparql/evaluate.py:562\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevalProject\u001b[39m(ctx: QueryContext, project: CompValue):\n\u001b[1;32m    561\u001b[0m     res \u001b[38;5;241m=\u001b[39m evalPart(ctx, project\u001b[38;5;241m.\u001b[39mp)\n\u001b[0;32m--> 562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (row\u001b[38;5;241m.\u001b[39mproject(project\u001b[38;5;241m.\u001b[39mPV) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m res)\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.9/site-packages/rdflib/plugins/sparql/evaluate.py:112\u001b[0m, in \u001b[0;36mevalBGP\u001b[0;34m(ctx, bgp)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AlreadyBound:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m evalBGP(c, bgp[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.9/site-packages/rdflib/plugins/sparql/evaluate.py:112\u001b[0m, in \u001b[0;36mevalBGP\u001b[0;34m(ctx, bgp)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AlreadyBound:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m evalBGP(c, bgp[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.9/site-packages/rdflib/plugins/sparql/evaluate.py:90\u001b[0m, in \u001b[0;36mevalBGP\u001b[0;34m(ctx, bgp)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ss, sp, so \u001b[38;5;129;01min\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mtriples((_s, _p, _o)):  \u001b[38;5;66;03m# type: ignore[union-attr, arg-type]\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (_s, _p, _o):\n\u001b[0;32m---> 90\u001b[0m         c \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m         c \u001b[38;5;241m=\u001b[39m ctx\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.9/site-packages/rdflib/plugins/sparql/sparql.py:396\u001b[0m, in \u001b[0;36mQueryContext.push\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpush\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryContext\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 396\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBindings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.9/site-packages/rdflib/plugins/sparql/sparql.py:293\u001b[0m, in \u001b[0;36mQueryContext.clone\u001b[0;34m(self, bindings)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclone\u001b[39m(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m, bindings: Optional[Union[FrozenBindings, Bindings, List[Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    292\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryContext\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 293\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mQueryContext\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitBindings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitBindings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     r\u001b[38;5;241m.\u001b[39mprologue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprologue\n\u001b[1;32m    299\u001b[0m     r\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.9/site-packages/rdflib/plugins/sparql/sparql.py:280\u001b[0m, in \u001b[0;36mQueryContext.__init__\u001b[0;34m(self, graph, bindings, initBindings)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprologue: Optional[Prologue] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_now: Optional[datetime\u001b[38;5;241m.\u001b[39mdatetime] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnodes: t\u001b[38;5;241m.\u001b[39mMutableMapping[Identifier, BNode] \u001b[38;5;241m=\u001b[39m \u001b[43mcollections\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaultdict\u001b[49m(\n\u001b[1;32m    281\u001b[0m     BNode\n\u001b[1;32m    282\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e06dc2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: 'mortar_bricked'\n",
      "Loading 'Brick.ttl' into memory...\n",
      "Brick graph loaded successfully.\n",
      "\n",
      "Found 44 building graphs to process in 'mortar_updated2'.\n",
      "\n",
      "--- Starting merge process ---\n",
      "  - Processing 'bldg5.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg5.ttl'\n",
      "  - Processing 'bldg4.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg4.ttl'\n",
      "  - Processing 'bldg6.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg6.ttl'\n",
      "  - Processing 'bldg44.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg44.ttl'\n",
      "  - Processing 'bldg7.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg7.ttl'\n",
      "  - Processing 'bldg3.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg3.ttl'\n",
      "  - Processing 'bldg41.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg41.ttl'\n",
      "  - Processing 'bldg40.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg40.ttl'\n",
      "  - Processing 'bldg2.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg2.ttl'\n",
      "  - Processing 'bldg42.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg42.ttl'\n",
      "  - Processing 'bldg43.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg43.ttl'\n",
      "  - Processing 'bldg1.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg1.ttl'\n",
      "  - Processing 'bldg18.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg18.ttl'\n",
      "  - Processing 'bldg24.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg24.ttl'\n",
      "  - Processing 'bldg31.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg31.ttl'\n",
      "  - Processing 'bldg25.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg25.ttl'\n",
      "  - Processing 'bldg19.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg19.ttl'\n",
      "  - Processing 'bldg33.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg33.ttl'\n",
      "  - Processing 'bldg27.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg27.ttl'\n",
      "  - Processing 'bldg26.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg26.ttl'\n",
      "  - Processing 'bldg32.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg32.ttl'\n",
      "  - Processing 'smc.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/smc.ttl'\n",
      "  - Processing 'bldg36.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg36.ttl'\n",
      "  - Processing 'bldg22.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg22.ttl'\n",
      "  - Processing 'bldg23.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg23.ttl'\n",
      "  - Processing 'bldg37.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg37.ttl'\n",
      "  - Processing 'bldg21.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg21.ttl'\n",
      "  - Processing 'bldg35.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg35.ttl'\n",
      "  - Processing 'bldg34.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg34.ttl'\n",
      "  - Processing 'bldg20.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg20.ttl'\n",
      "  - Processing 'bldg39.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg39.ttl'\n",
      "  - Processing 'bldg11.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg11.ttl'\n",
      "  - Processing 'bldg10.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg10.ttl'\n",
      "  - Processing 'bldg38.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg38.ttl'\n",
      "  - Processing 'bldg12.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg12.ttl'\n",
      "  - Processing 'bldg13.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg13.ttl'\n",
      "  - Processing 'bldg17.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg17.ttl'\n",
      "  - Processing 'bldg16.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg16.ttl'\n",
      "  - Processing 'bldg14.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg14.ttl'\n",
      "  - Processing 'bldg28.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg28.ttl'\n",
      "  - Processing 'bldg29.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg29.ttl'\n",
      "  - Processing 'bldg15.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg15.ttl'\n",
      "  - Processing 'bldg9.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg9.ttl'\n",
      "  - Processing 'bldg8.ttl'...\n",
      "    -> Saved combined graph to 'mortar_bricked/bldg8.ttl'\n",
      "\n",
      "--- Process complete ---\n"
     ]
    }
   ],
   "source": [
    "# You must have the required libraries installed:\n",
    "# pip install rdflib\n",
    "\n",
    "import os\n",
    "import rdflib\n",
    "\n",
    "# --- Configuration ---\n",
    "source_directory = \"mortar_updated2\"\n",
    "brick_file_path = \"Brick.ttl\" \n",
    "destination_directory = \"mortar_bricked\"\n",
    "# --- End Configuration ---\n",
    "\n",
    "# 1. Create the destination directory if it doesn't exist\n",
    "if not os.path.exists(destination_directory):\n",
    "    os.makedirs(destination_directory)\n",
    "    print(f\"Created directory: '{destination_directory}'\")\n",
    "\n",
    "# 2. Load the Brick graph into memory once for efficiency\n",
    "print(f\"Loading '{brick_file_path}' into memory...\")\n",
    "brick_graph = rdflib.Graph()\n",
    "try:\n",
    "    brick_graph.parse(brick_file_path, format='turtle')\n",
    "    print(\"Brick graph loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{brick_file_path}' was not found. Please ensure it's in the correct location.\")\n",
    "    # Exit the script if Brick.ttl is not found\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred parsing '{brick_file_path}': {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# 3. Get a list of all .ttl files from the source directory\n",
    "try:\n",
    "    all_files = os.listdir(source_directory)\n",
    "    ttl_files = [f for f in all_files if f.endswith('.ttl')]\n",
    "    print(f\"\\nFound {len(ttl_files)} building graphs to process in '{source_directory}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The source directory '{source_directory}' was not found.\")\n",
    "    exit()\n",
    "\n",
    "# 4. Loop through each building graph, merge with Brick, and save\n",
    "print(\"\\n--- Starting merge process ---\")\n",
    "for ttl_filename in ttl_files:\n",
    "    source_file_path = os.path.join(source_directory, ttl_filename)\n",
    "    destination_file_path = os.path.join(destination_directory, ttl_filename)\n",
    "    \n",
    "    print(f\"  - Processing '{ttl_filename}'...\")\n",
    "    \n",
    "    # Create a new graph for the building data\n",
    "    building_graph = rdflib.Graph()\n",
    "    \n",
    "    # Parse the building's TTL file\n",
    "    building_graph.parse(source_file_path, format='turtle')\n",
    "    \n",
    "    # Merge the building graph with the pre-loaded Brick graph\n",
    "    # The '+' operator in rdflib combines the two graphs\n",
    "    combined_graph = building_graph + brick_graph\n",
    "    \n",
    "    # Serialize and save the combined graph to the new directory\n",
    "    combined_graph.serialize(destination=destination_file_path, format='turtle')\n",
    "    print(f\"    -> Saved combined graph to '{destination_file_path}'\")\n",
    "\n",
    "print(\"\\n--- Process complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb5998c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Analyzing Triple Counts in 'mortar_bricked' ---\n",
      "Analyzed 44 building graphs.\n",
      "\n",
      "--- Step 2: Loading queries from 'mortar.json' ---\n",
      "Loaded 9 queries.\n",
      "\n",
      "--- Step 3: Finding the most successful buildings ---\n",
      "Processing query: MORTAR_001\n",
      "Processing query: MORTAR_002\n",
      "Processing query: MORTAR_003\n",
      "Processing query: MORTAR_004\n",
      "Processing query: MORTAR_005\n",
      "Processing query: MORTAR_006\n",
      "Processing query: MORTAR_007\n",
      "Processing query: MORTAR_008\n",
      "Processing query: MORTAR_009\n",
      "\n",
      "--- Step 4: Analysis of Top-Performing Buildings ---\n",
      " à¤¬à¤¿à¤²à¥à¤¡à¤¿à¤‚à¤—(s) that returned results for the most SPARQL queries (9 queries) are:\n",
      "  - bldg5.ttl\n",
      "  - bldg4.ttl\n",
      "  - bldg44.ttl\n",
      "  - bldg40.ttl\n",
      "  - bldg18.ttl\n",
      "  - bldg39.ttl\n",
      "  - bldg11.ttl\n",
      "  - bldg13.ttl\n",
      "\n",
      "--- Triple Count Statistics for These Top Buildings ---\n",
      "  Minimum Triples: 54408\n",
      "  Median Triples:  55516\n",
      "  Maximum Triples: 62578\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import rdflib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "graphs_directory = \"mortar_bricked\"\n",
    "json_file = \"mortar.json\"\n",
    "# --- End Configuration ---\n",
    "\n",
    "# 1. Calculate triple counts for all TTL files for later use.\n",
    "print(f\"--- Step 1: Analyzing Triple Counts in '{graphs_directory}' ---\")\n",
    "file_triple_counts = []\n",
    "all_files = os.listdir(graphs_directory)\n",
    "ttl_files = [f for f in all_files if f.endswith('.ttl')]\n",
    "\n",
    "for ttl_filename in ttl_files:\n",
    "    file_path = os.path.join(graphs_directory, ttl_filename)\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(file_path, format='turtle')\n",
    "    file_triple_counts.append({'filename': ttl_filename, 'triples': len(g)})\n",
    "\n",
    "counts_df = pd.DataFrame(file_triple_counts)\n",
    "print(f\"Analyzed {len(counts_df)} building graphs.\\n\")\n",
    "\n",
    "\n",
    "# 2. Load the queries from the JSON file.\n",
    "print(f\"--- Step 2: Loading queries from '{json_file}' ---\")\n",
    "with open(json_file, 'r') as f:\n",
    "    queries_data = json.load(f)\n",
    "print(f\"Loaded {len(queries_data)} queries.\\n\")\n",
    "\n",
    "\n",
    "# 3. Find the buildings that are successful for the most queries.\n",
    "print(\"--- Step 3: Finding the most successful buildings ---\")\n",
    "# Use a dictionary to keep track of how many queries each building answers.\n",
    "building_success_counts = {filename: 0 for filename in ttl_files}\n",
    "\n",
    "# Loop through each query\n",
    "for query_info in queries_data:\n",
    "    print(f\"Processing query: {query_info.get('query_id', 'N/A')}\")\n",
    "    sparql_query = query_info['sparql_query']\n",
    "    \n",
    "    # Then loop through each building graph\n",
    "    for ttl_filename in ttl_files:\n",
    "        file_path = os.path.join(graphs_directory, ttl_filename)\n",
    "        g = rdflib.Graph()\n",
    "        g.parse(file_path, format='turtle')\n",
    "        \n",
    "        try:\n",
    "            results = g.query(sparql_query)\n",
    "            if len(results) > 0:\n",
    "                # If the query returns results, increment the counter for that building\n",
    "                building_success_counts[ttl_filename] += 1\n",
    "        except Exception as e:\n",
    "            # In case a SPARQL query has a syntax error\n",
    "            print(f\"Could not execute query {query_info.get('query_id', 'N/A')} on {ttl_filename}: {e}\")\n",
    "\n",
    "# Find the highest number of successful queries\n",
    "max_success_count = 0\n",
    "if building_success_counts:\n",
    "    max_success_count = max(building_success_counts.values())\n",
    "\n",
    "# Identify all buildings that have this maximum success count\n",
    "top_buildings = [building for building, count in building_success_counts.items()\n",
    "                 if count == max_success_count]\n",
    "\n",
    "# --- Final Analysis ---\n",
    "print(f\"\\n--- Step 4: Analysis of Top-Performing Buildings ---\")\n",
    "\n",
    "if not top_buildings:\n",
    "    print(\"No buildings returned results for any of the queries.\")\n",
    "else:\n",
    "    # Print the primary result: the most successful buildings\n",
    "    print(f\" à¤¬à¤¿à¤²à¥à¤¡à¤¿à¤‚à¤—(s) that returned results for the most SPARQL queries ({max_success_count} queries) are:\")\n",
    "    for building in top_buildings:\n",
    "        print(f\"  - {building}\")\n",
    "\n",
    "    # Now, get the triple counts for just these top buildings\n",
    "    top_buildings_df = counts_df[counts_df['filename'].isin(top_buildings)]\n",
    "    \n",
    "    # Calculate and print the required statistics\n",
    "    min_triples = top_buildings_df['triples'].min()\n",
    "    median_triples = top_buildings_df['triples'].median()\n",
    "    max_triples = top_buildings_df['triples'].max()\n",
    "\n",
    "    print(\"\\n--- Triple Count Statistics for These Top Buildings ---\")\n",
    "    print(f\"  Minimum Triples: {min_triples}\")\n",
    "    print(f\"  Median Triples:  {int(median_triples)}\")\n",
    "    print(f\"  Maximum Triples: {max_triples}\")\n",
    "    print(\"-------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe82c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Tests on 4 Buildings ---\n",
      "\n",
      "--- Processing Building: bestest_air_with_data.ttl ---\n",
      "Graph loaded successfully. Found 132 triples.\n",
      "  - SUCCESS: DFLEXLIBS_001_Zone_Info returned 216 results.\n",
      "  - SUCCESS: DFLEXLIBS_002_AHU_Info returned 1 results.\n",
      "  - SUCCESS: DFLEXLIBS_003_Power_Sensors returned 2 results.\n",
      "  - SUCCESS: DFLEXLIBS_004_Building_Area returned 1 results.\n",
      "  - NO RESULT: DFLEXLIBS_005_Electric_Power_Baseline\n",
      "  - NO RESULT: DFLEXLIBS_005_Electric_Power_Flexible\n",
      "  - NO RESULT: DFLEXLIBS_006_Thermal_Power_Baseline\n",
      "  - NO RESULT: DFLEXLIBS_006_Thermal_Power_Flexible\n",
      "\n",
      "* Summary for bestest_air_with_data.ttl:\n",
      "  - Total Triples: 132\n",
      "  - Successful Queries: 4 / 8\n",
      "-----------------------------------\n",
      "\n",
      "--- Processing Building: bestest_hydronic_heat_pump_with_data.ttl ---\n",
      "Graph loaded successfully. Found 102 triples.\n",
      "  - SUCCESS: DFLEXLIBS_001_Zone_Info returned 72 results.\n",
      "  - SUCCESS: DFLEXLIBS_002_AHU_Info returned 1 results.\n",
      "  - SUCCESS: DFLEXLIBS_003_Power_Sensors returned 3 results.\n",
      "  - SUCCESS: DFLEXLIBS_004_Building_Area returned 1 results.\n",
      "  - NO RESULT: DFLEXLIBS_005_Electric_Power_Baseline\n",
      "  - NO RESULT: DFLEXLIBS_005_Electric_Power_Flexible\n",
      "  - NO RESULT: DFLEXLIBS_006_Thermal_Power_Baseline\n",
      "  - NO RESULT: DFLEXLIBS_006_Thermal_Power_Flexible\n",
      "\n",
      "* Summary for bestest_hydronic_heat_pump_with_data.ttl:\n",
      "  - Total Triples: 102\n",
      "  - Successful Queries: 4 / 8\n",
      "-----------------------------------\n",
      "\n",
      "--- Processing Building: multizone_office_simple_air_with_data.ttl ---\n",
      "Graph loaded successfully. Found 629 triples.\n",
      "  - SUCCESS: DFLEXLIBS_001_Zone_Info returned 1080 results.\n",
      "  - SUCCESS: DFLEXLIBS_002_AHU_Info returned 5 results.\n",
      "  - SUCCESS: DFLEXLIBS_003_Power_Sensors returned 7 results.\n",
      "  - SUCCESS: DFLEXLIBS_004_Building_Area returned 1 results.\n",
      "  - NO RESULT: DFLEXLIBS_005_Electric_Power_Baseline\n",
      "  - NO RESULT: DFLEXLIBS_005_Electric_Power_Flexible\n",
      "  - NO RESULT: DFLEXLIBS_006_Thermal_Power_Baseline\n",
      "  - NO RESULT: DFLEXLIBS_006_Thermal_Power_Flexible\n",
      "\n",
      "* Summary for multizone_office_simple_air_with_data.ttl:\n",
      "  - Total Triples: 629\n",
      "  - Successful Queries: 4 / 8\n",
      "-----------------------------------\n",
      "\n",
      "--- Processing Building: singlezone_commercial_hydronic_with_data.ttl ---\n",
      "Graph loaded successfully. Found 141 triples.\n",
      "  - SUCCESS: DFLEXLIBS_001_Zone_Info returned 72 results.\n",
      "  - SUCCESS: DFLEXLIBS_002_AHU_Info returned 1 results.\n",
      "  - SUCCESS: DFLEXLIBS_003_Power_Sensors returned 5 results.\n",
      "  - SUCCESS: DFLEXLIBS_004_Building_Area returned 1 results.\n",
      "  - NO RESULT: DFLEXLIBS_005_Electric_Power_Baseline\n",
      "  - NO RESULT: DFLEXLIBS_005_Electric_Power_Flexible\n",
      "  - NO RESULT: DFLEXLIBS_006_Thermal_Power_Baseline\n",
      "  - NO RESULT: DFLEXLIBS_006_Thermal_Power_Flexible\n",
      "\n",
      "* Summary for singlezone_commercial_hydronic_with_data.ttl:\n",
      "  - Total Triples: 141\n",
      "  - Successful Queries: 4 / 8\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import rdflib\n",
    "\n",
    "def run_building_tests(buildings_to_test, queries_to_run, ttl_directory=\".\"):\n",
    "    \"\"\"\n",
    "    Tests a list of buildings against a hardcoded list of SPARQL queries.\n",
    "\n",
    "    For each building, it loads the TTL graph and executes all provided \n",
    "    queries against it, reporting the results.\n",
    "\n",
    "    Args:\n",
    "        buildings_to_test (list): A list of building names (e.g., \"bestest_air_with_data\").\n",
    "        queries_to_run (list): A list of SPARQL query objects (dictionaries).\n",
    "        ttl_directory (str): The directory where the .ttl files are located.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Tests on {len(buildings_to_test)} Buildings ---\")\n",
    "    \n",
    "    for building_name in buildings_to_test:\n",
    "        building_ttl_file = f\"{building_name}.ttl\"\n",
    "        building_path = os.path.join(ttl_directory, building_ttl_file)\n",
    "        \n",
    "        print(f\"\\n--- Processing Building: {building_ttl_file} ---\")\n",
    "\n",
    "        # --- Load Graph and Count Triples ---\n",
    "        try:\n",
    "            g = rdflib.Graph()\n",
    "            g.parse(building_path, format='turtle')\n",
    "            num_triples = len(g)\n",
    "            print(f\"Graph loaded successfully. Found {num_triples} triples.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: TTL file not found at '{building_path}'. Skipping.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing '{building_path}': {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # --- Execute Applicable Queries ---\n",
    "        successful_query_count = 0\n",
    "        for query_obj in queries_to_run:\n",
    "            query_id = query_obj.get('query_id')\n",
    "            sparql_query = query_obj.get('sparql_query')\n",
    "            \n",
    "            try:\n",
    "                results = g.query(sparql_query)\n",
    "                if len(results) > 0:\n",
    "                    successful_query_count += 1\n",
    "                    print(f\"  - SUCCESS: {query_id} returned {len(results)} results.\")\n",
    "                else:\n",
    "                    print(f\"  - NO RESULT: {query_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - ERROR executing {query_id}: {e}\")\n",
    "\n",
    "        # --- Report Summary for the Building ---\n",
    "        print(f\"\\n* Summary for {building_ttl_file}:\")\n",
    "        print(f\"  - Total Triples: {num_triples}\")\n",
    "        print(f\"  - Successful Queries: {successful_query_count} / {len(queries_to_run)}\")\n",
    "        print(\"-\" * 35)\n",
    "\n",
    "# --- Configuration ---\n",
    "# List of buildings to test (must match the filenames)\n",
    "BUILDINGS_TO_TEST = [\n",
    "    \"bestest_air_with_data\",\n",
    "    \"bestest_hydronic_heat_pump_with_data\",\n",
    "    \"multizone_office_simple_air_with_data\",\n",
    "    \"singlezone_commercial_hydronic_with_data\"\n",
    "]\n",
    "\n",
    "# Hardcoded list of SPARQL queries\n",
    "QUERIES = [\n",
    "    {\n",
    "        \"query_id\": \"DFLEXLIBS_001_Zone_Info\",\n",
    "        \"sparql_query\": \"\"\"\n",
    "SELECT ?zone_name ?zone_temp_point ?set_temp_min_point ?set_temp_max_point ?occ_sensor_point ?occ_cmd_point \n",
    "?zone_set_temp_heat_point  ?zone_set_temp_cool_point ?unocc_zone_set_temp_heat_point ?unocc_zone_set_temp_cool_point \n",
    "?occ_zone_set_temp_heat_point ?occ_zone_set_temp_cool_point  ?zone_set_temp_point ?vav_damper_set_point ?vav_discharge_temp_point \n",
    "?vav_reheat_command_point\n",
    "\n",
    "WHERE {\n",
    "    ?zone a brick:HVAC_Zone  ;\n",
    "    rdfs:label ?zone_name.\n",
    "   \n",
    "    OPTIONAL { \n",
    "        ?zone     brick:hasPoint ?TZon.\n",
    "        ?TZon a brick:Zone_Air_Temperature_Sensor ;\n",
    "            ref:hasExternalReference/ref:hasTimeseriesId ?zone_temp_point .}\n",
    "    \n",
    "    OPTIONAL {\n",
    "        ?zone brick:hasPoint ?TSetMin .\n",
    "        ?TSetMin a brick:Min_Air_Temperature_Setpoint ;\n",
    "                ref:hasExternalReference/ref:hasTimeseriesId ?set_temp_min_point .}\n",
    "\n",
    "    OPTIONAL {\n",
    "        ?zone brick:hasPoint ?TSetMax .\n",
    "        ?TSetMax a brick:Max_Air_Temperature_Setpoint ;\n",
    "                    ref:hasExternalReference/ref:hasTimeseriesId ?set_temp_max_point .}\n",
    "    \n",
    "    OPTIONAL {\n",
    "        ?zone brick:hasPoint ?TSetHeaZon .\n",
    "        ?TSetHeaZon a brick:Zone_Air_Heating_Temperature_Setpoint;\n",
    "                    ref:hasExternalReference/ref:hasTimeseriesId ?zone_set_temp_heat_point .}\n",
    "\n",
    "    OPTIONAL {\n",
    "        ?zone brick:hasPoint ?TSetCooZon .\n",
    "        ?TSetCooZon a brick:Zone_Air_Cooling_Temperature_Setpoint;\n",
    "                    ref:hasExternalReference/ref:hasTimeseriesId ?zone_set_temp_cool_point .}\n",
    "\n",
    "\n",
    "    OPTIONAL {\n",
    "        ?zone brick:hasPoint ?OccSensor.\n",
    "        ?OccSensor a brick:Occupancy_Sensor ;\n",
    "                   ref:hasExternalReference/ref:hasTimeseriesId ?occ_sensor_point .}\n",
    "\n",
    "    OPTIONAL {\n",
    "        ?zone brick:hasPoint ?OccCmd .\n",
    "        ?OccCmd a brick:Occupancy_Command ;\n",
    "                ref:hasExternalReference/ref:hasTimeseriesId ?occ_cmd_point .}\n",
    "    \n",
    "    OPTIONAL { \n",
    "         ?zone     brick:hasPoint ?UnOccTSetCooZon.\n",
    "        ?UnOccTSetCooZon a brick:Unoccupied_Cooling_Temperature_Setpoint ;\n",
    "                          ref:hasExternalReference/ref:hasTimeseriesId ?unocc_zone_set_temp_cool_point . \n",
    "    }\n",
    "    \n",
    "    OPTIONAL {\n",
    "        ?zone brick:hasPoint ?UnOccTSetHeaZon .\n",
    "        ?UnOccTSetHeaZon a brick:Unoccupied_Heating_Temperature_Setpoint;\n",
    "                         ref:hasExternalReference/ref:hasTimeseriesId ?unocc_zone_set_temp_heat_point .}\n",
    "    OPTIONAL {\n",
    "        ?zone brick:hasPoint ?OccTSetCooZon .\n",
    "        ?OccTSetCooZon a brick:Occupied_Cooling_Temperature_Setpoint;\n",
    "                       ref:hasExternalReference/ref:hasTimeseriesId ?occ_zone_set_temp_cool_point .}\n",
    "\n",
    "    OPTIONAL { \n",
    "        ?zone brick:hasPoint ?OccTSetHeaZon . \n",
    "        ?OccTSetHeaZon a brick:Occupied_Heating_Temperature_Setpoint ;\n",
    "                        ref:hasExternalReference/ref:hasTimeseriesId ?occ_zone_set_temp_heat_point . \n",
    "    }\n",
    "    \n",
    "    OPTIONAL {\n",
    "        ?zone brick:hasPoint ?TSetZon .\n",
    "        ?TSetZon a brick:Zone_Air_Temperature_Setpoint;\n",
    "                 ref:hasExternalReference/ref:hasTimeseriesId ?zone_set_temp_point .}\n",
    "    \n",
    "    OPTIONAL {\n",
    "        ?vav brick:feeds ?zone ;\n",
    "             brick:hasPoint ?Vav_DamSet .\n",
    "        ?Vav_DamSet a brick:Damper_Position_Setpoint;\n",
    "                    ref:hasExternalReference/ref:hasTimeseriesId ?vav_damper_set_point .}\n",
    "\n",
    "    OPTIONAL {\n",
    "        ?vav brick:feeds ?zone ;\n",
    "             brick:hasPoint ?Vav_DisTemp .\n",
    "        ?Vav_DisTemp a brick:Discharge_Air_Temperature_Sensor;\n",
    "                     ref:hasExternalReference/ref:hasTimeseriesId ?vav_discharge_temp_point .}\n",
    "    \n",
    "    OPTIONAL {\n",
    "        ?vav brick:feeds ?zone ;\n",
    "             brick:hasPoint ?Vav_ReheatCmd .\n",
    "        ?Vav_ReheatCmd a brick:Valve_Command;\n",
    "                       ref:hasExternalReference/ref:hasTimeseriesId ?vav_reheat_command_point .}\n",
    "\n",
    "\n",
    "}    \n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"DFLEXLIBS_002_AHU_Info\",\n",
    "        \"sparql_query\": \"\"\"\n",
    "SELECT ?ahu_supply_temp_point ?ahu_supply_flow_point ?ahu_supply_flow_set_point\n",
    "WHERE {\n",
    "    \n",
    "    OPTIONAL {\n",
    "    ?ahu brick:feeds/brick:feeds ?zone ;\n",
    "         brick:hasPoint ?Ahu_TSup .\n",
    "    ?Ahu_TSup a brick:Supply_Air_Temperature_Sensor;\n",
    "        ref:hasExternalReference/ref:hasTimeseriesId ?ahu_supply_temp_point .}\n",
    "\n",
    "    OPTIONAL {\n",
    "    ?ahu brick:feeds/brick:feeds ?zone ;\n",
    "         brick:hasPoint ?Ahu_flowSup .\n",
    "    ?Ahu_flowSup a brick:Supply_Air_Flow_Sensor;\n",
    "        ref:hasExternalReference/ref:hasTimeseriesId ?ahu_supply_flow_point .}\n",
    "\n",
    "    OPTIONAL {\n",
    "    ?ahu brick:feeds/brick:feeds ?zone ;\n",
    "         brick:hasPoint ?Ahu_flowSupSet .\n",
    "    ?Ahu_flowSupSet a brick:Supply_Air_Flow_Setpoint;\n",
    "        ref:hasExternalReference/ref:hasTimeseriesId ?ahu_supply_flow_set_point .}\n",
    "\n",
    "}\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"DFLEXLIBS_003_Power_Sensors\",\n",
    "        \"sparql_query\": \"\"\"\n",
    "SELECT DISTINCT ?ele_pow_point ?therm_pow_point\n",
    "WHERE {\n",
    "    \n",
    "        OPTIONAL {\n",
    "    ?Epower a brick:Electric_Power_Sensor ;\n",
    "               ref:hasExternalReference/ref:hasTimeseriesId ?ele_pow_point .}\n",
    "\n",
    "    OPTIONAL {\n",
    "    ?Tpower a brick:Thermal_Power_Sensor ;\n",
    "               ref:hasExternalReference/ref:hasTimeseriesId ?therm_pow_point .}\n",
    "\n",
    "\n",
    "}\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"DFLEXLIBS_004_Building_Area\",\n",
    "        \"sparql_query\": \"\"\"\n",
    "SELECT ?area WHERE {\n",
    "    ?building a brick:Building ;\n",
    "    brick:area [brick:value ?area] .\n",
    "\n",
    "}\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"DFLEXLIBS_005_Electric_Power_Baseline\",\n",
    "        \"sparql_query\": \"\"\"\n",
    "            PREFIX brick: <https://brickschema.org/schema/Brick#>\n",
    "            PREFIX ref: <https://brickschema.org/schema/Brick/ref#>\n",
    "            SELECT ?power_point WHERE {\n",
    "                ?point a brick:Electric_Power_Sensor ;\n",
    "                ref:hasExternalReference ?ref .\n",
    "                ?ref ref:onTable \"baseline_\" ;\n",
    "                ref:hasTimeseriesId ?power_point .\n",
    "            }\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"DFLEXLIBS_005_Electric_Power_Flexible\",\n",
    "        \"sparql_query\": \"\"\"\n",
    "            PREFIX brick: <https://brickschema.org/schema/Brick#>\n",
    "            PREFIX ref: <https://brickschema.org/schema/Brick/ref#>\n",
    "            SELECT ?power_point WHERE {\n",
    "                ?point a brick:Electric_Power_Sensor ;\n",
    "                ref:hasExternalReference ?ref .\n",
    "                ?ref ref:onTable \"flexible_\" ;\n",
    "                ref:hasTimeseriesId ?power_point .\n",
    "            }\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"DFLEXLIBS_006_Thermal_Power_Baseline\",\n",
    "        \"sparql_query\": \"\"\"\n",
    "            PREFIX brick: <https://brickschema.org/schema/Brick#>\n",
    "            PREFIX ref: <https://brickschema.org/schema/Brick/ref#>\n",
    "            SELECT ?power_point WHERE {\n",
    "                ?point a brick:Thermal_Power_Sensor ;\n",
    "                ref:hasExternalReference ?ref .\n",
    "                ?ref ref:onTable \"baseline_\" ;\n",
    "                ref:hasTimeseriesId ?power_point .\n",
    "            }\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"DFLEXLIBS_006_Thermal_Power_Flexible\",\n",
    "        \"sparql_query\": \"\"\"\n",
    "            PREFIX brick: <https://brickschema.org/schema/Brick#>\n",
    "            PREFIX ref: <https://brickschema.org/schema/Brick/ref#>\n",
    "            SELECT ?power_point WHERE {\n",
    "                ?point a brick:Thermal_Power_Sensor ;\n",
    "                ref:hasExternalReference ?ref .\n",
    "                ?ref ref:onTable \"flexible_\" ;\n",
    "                ref:hasTimeseriesId ?power_point .\n",
    "            }\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Directory containing the .ttl files for the buildings.\n",
    "# Assumes they are in the same directory as the notebook.\n",
    "# Change this if your files are elsewhere, e.g., \"path/to/your/ttl/files\"\n",
    "TTL_FILES_DIRECTORY = \".\" \n",
    "\n",
    "# --- Execution ---\n",
    "run_building_tests(BUILDINGS_TO_TEST, QUERIES, TTL_FILES_DIRECTORY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4101c79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'dflexlibs_multizone.json'...\n",
      "Reading generated questions from 'dflexlibs_multizone_generated_questions.csv'...\n",
      "Successfully merged questions.\n",
      "New file saved to 'combined_data/dflexlibs_multizone_combined.json'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d2318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
