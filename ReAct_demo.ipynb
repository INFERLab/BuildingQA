{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf03f3f",
   "metadata": {},
   "source": [
    "# Knowledge Graph Question Answering for Building Knowledge Graphs using a ReAct Framework\n",
    "\n",
    "This notebook implements an agentic workflow based on the **ReAct (Reasoning and Acting)** framework to translate natural language questions into precise SPARQL queries. The core of this notebook is a multi-turn refinement process where two specialized agents collaborate to produce, critique, and improve a SPARQL query until it is deemed correct.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ Methodology: The Two-Agent Loop\n",
    "\n",
    "The agentic workflow is orchestrated by the `SparqlRefinementAgent` class and consists of an iterative loop between two Large Language Model (LLM) powered agents:\n",
    "\n",
    "1.  **Query Writer Agent**:\n",
    "    * **Role**: To generate and revise SPARQL queries.\n",
    "    * **Action**: Given a natural language question and a subset (defined by `num_triples` of the graph) of the graph, it writes an initial SPARQL query. In subsequent turns, it revises the query based on feedback.\n",
    "\n",
    "2.  **Critique Agent**:\n",
    "    * **Role**: To evaluate the query's correctness.\n",
    "    * **Action**: After the Writer's query is executed, the Critique agent reviews the original question, the query itself, and a summary of the execution results. It then makes a decision:\n",
    "        * `FINAL`: The query is correct and successfully answers the question. The loop terminates.\n",
    "        * `IMPROVE`: The query is incorrect, incomplete, or inefficient. The agent provides specific, actionable feedback.\n",
    "\n",
    "This feedback is then passed back to the Query Writer Agent, which begins the next iteration by generating an improved query. This cycle continues until a `FINAL` decision is reached or the maximum number of iterations is exceeded.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Execution Workflow\n",
    "\n",
    "The notebook follows a systematic process for each building and question:\n",
    "\n",
    "1.  **Configuration**: Configure your API keys and URL for LLM calls.\n",
    "2.  **Define Helper Functions**: Helper functions for logging data and extracting a subset of the graph is defined.\n",
    "3.  **Initialize Agents**: The `SparqlRefinementAgent` is instantiated.\n",
    "4.  **Define a helper function for testing a single question**: `run_single_question` function is defined for initial testing.\n",
    "5.  **Configure the necessary test conditions**: Key parameters such as the target building (`BUILDING_NAME`), LLM (`MODEL_NAME`), and SPARQL endpoint are set.\n",
    "4.  **Process All Questions and Buildings**: The script iterates through each question in the JSON file.\n",
    "    * The **ReAct loop** is triggered for the current question.\n",
    "    * The final query generated by the agent is executed.\n",
    "    * The ground-truth query is executed for comparison.\n",
    "    * Key metrics are computed and saved. \n",
    "\n",
    "---\n",
    "\n",
    "## üìä Evaluation Metrics\n",
    "\n",
    "To rigorously assess the correctness of the generated SPARQL query, the following metrics are calculated and logged:\n",
    "\n",
    "* **Arity Matching F1**: Checks if the query returned the correct **number of columns**.\n",
    "* **Exact Match F1**: A strict check for identical row content and column **order**, though it ignores column names.\n",
    "* **Entity Set F1**: Flexibly checks if the correct **sets of values** were retrieved in each column, regardless of row structure or column order.\n",
    "* **Row Matching F1**: The most robust metric. It finds the optimal column alignment and then checks for an exact, row-for-row match of the content. This is the primary indicator of a perfectly correct query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import traceback\n",
    "import uuid\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from pyparsing import ParseException\n",
    "from rdflib import BNode, Graph, Literal, URIRef\n",
    "from SPARQLWrapper import JSON, SPARQLWrapper\n",
    "from metrics import (\n",
    "    get_arity_matching_f1,\n",
    "    get_entity_set_f1,\n",
    "    get_row_matching_f1,\n",
    "    get_exact_match_f1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbd8bf",
   "metadata": {},
   "source": [
    "### 1. Assign your API key and your base url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750f3ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\" # Change this to your actual API key for whatever service you are using or set it in your environment variables\n",
    "client = OpenAI(    \n",
    "    api_key=os.environ.get('OPENAI_API_KEY'),\n",
    "    base_url=\"https://api.openai.com/v1/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d5af7",
   "metadata": {},
   "source": [
    "### 2. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3604d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_kg_subset_content(original_ttl_path: str, max_triples: int) -> str:\n",
    "    \"\"\"\n",
    "    Parses a TTL file and returns a string containing the prefixes and the first `max_triples`.\n",
    "    If the graph is smaller than max_triples, it returns the full content.\n",
    "    \"\"\"\n",
    "    full_graph = Graph()\n",
    "    try:\n",
    "        full_graph.parse(original_ttl_path, format=\"turtle\")\n",
    "        print(f\"üîé Original graph '{os.path.basename(original_ttl_path)}' contains {len(full_graph)} triples.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: Could not parse original TTL file at {original_ttl_path}. Reason: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    # If the graph is small enough, use the whole thing\n",
    "    if len(full_graph) <= max_triples:\n",
    "        print(f\"   -> Graph has {len(full_graph)} triples or fewer. Using full graph content for prompt.\")\n",
    "        return full_graph.serialize(format=\"turtle\")\n",
    "\n",
    "    print(f\"   -> Graph is larger than {max_triples} triples. Creating a subset for the prompt...\")\n",
    "    subset_graph = Graph()\n",
    "    # Copy namespaces to the subset graph\n",
    "    for prefix, namespace in full_graph.namespace_manager.namespaces():\n",
    "        subset_graph.bind(prefix, namespace)\n",
    "\n",
    "    # Add the first `max_triples`\n",
    "    for i, triple in enumerate(full_graph):\n",
    "        if i >= max_triples:\n",
    "            break\n",
    "        subset_graph.add(triple)\n",
    "\n",
    "    print(f\"   -> ‚úÖ Successfully created subset context with {len(subset_graph)} triples.\")\n",
    "    return subset_graph.serialize(format=\"turtle\")\n",
    "\n",
    "# --- Evaluation and Helper Functions ---\n",
    "\n",
    "def extract_prefixes_from_ttl(ttl_path: str) -> str:\n",
    "    \"\"\"Dynamically extracts PREFIX declarations from a TTL file.\"\"\"\n",
    "    prefixes = []\n",
    "    try:\n",
    "        with open(ttl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                stripped_line = line.strip()\n",
    "                if stripped_line.lower().startswith('@prefix'):\n",
    "                    parts = stripped_line.split()\n",
    "                    if len(parts) >= 3:\n",
    "                        prefixes.append(f\"PREFIX {parts[1]} {parts[2]}\")\n",
    "        print(f\"‚úÖ Successfully extracted {len(prefixes)} prefixes from {os.path.basename(ttl_path)}.\")\n",
    "        return \"\\n\".join(prefixes) + \"\\n\\n\"\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERROR: TTL file not found at {ttl_path}.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: Could not read prefixes from {ttl_path}. Reason: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def check_if_question_exists(question_text: str, log_filename: str, model_name: str) -> bool:\n",
    "    \"\"\"Checks if a question has already been logged for a specific model.\"\"\"\n",
    "    if not os.path.exists(log_filename):\n",
    "        return False\n",
    "    try:\n",
    "        with open(log_filename, 'r', newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                if row.get('question') == question_text and row.get('model') == model_name:\n",
    "                    print(f\"‚úÖ Result for question '{question_text[:50]}...' and model '{model_name}' already exists. Skipping.\")\n",
    "                    return True\n",
    "    except (FileNotFoundError, Exception) as e:\n",
    "        print(f\"Could not read log file {log_filename}. Error: {e}\")\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "# --- CSV Logger Class ---\n",
    "\n",
    "LOG_FIELDNAMES = [\n",
    "    'query_id', 'question_number', 'source', 'question', 'model',\n",
    "    'ground_truth_sparql', 'generated_sparql',\n",
    "    'syntax_ok', 'returns_results', 'perfect_match',\n",
    "    'gt_num_rows', 'gt_num_cols',\n",
    "    'gen_num_rows', 'gen_num_cols',\n",
    "    'arity_matching_f1',\n",
    "    'exact_match_f1',\n",
    "    'entity_set_f1',\n",
    "    'row_matching_f1',\n",
    "    'less_columns_flag',\n",
    "    'prompt_tokens', 'completion_tokens', 'total_tokens'\n",
    "]\n",
    "\n",
    "class CsvLogger:\n",
    "    \"\"\"Handles writing log data to a CSV file, appending if it exists.\"\"\"\n",
    "    def __init__(self, filename: str, fieldnames: List[str]):\n",
    "        self.filename = filename\n",
    "        self.fieldnames = fieldnames\n",
    "        \n",
    "        file_exists = os.path.exists(self.filename)\n",
    "        \n",
    "        self.file = open(self.filename, 'a', newline='', encoding='utf-8')\n",
    "        self.writer = csv.DictWriter(self.file, fieldnames=self.fieldnames)\n",
    "        \n",
    "        if not file_exists:\n",
    "            self.writer.writeheader()\n",
    "            print(f\"üìù New log file created. Writing to {self.filename}\")\n",
    "        else:\n",
    "            print(f\"üìù Logger initialized. Appending to {self.filename}\")\n",
    "\n",
    "    def log(self, data: Dict[str, Any]):\n",
    "        \"\"\"Writes a single entry to the log file.\"\"\"\n",
    "        self.writer.writerow(data)\n",
    "        self.file.flush()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the log file.\"\"\"\n",
    "        self.file.close()\n",
    "        print(f\"‚úÖ Logger closed. Final results saved to {self.filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97b556",
   "metadata": {},
   "source": [
    "### 3. Initialize The `SparqlRefinementAgent` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06694549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Pydantic Models for the Two-Agent Workflow ---\n",
    "\n",
    "class SparqlQuery(BaseModel):\n",
    "    \"\"\"Model for the Query Writer Agent's output.\"\"\"\n",
    "    sparql_query: str = Field(..., description=\"The generated or revised SPARQL query.\")\n",
    "\n",
    "class QueryCritique(BaseModel):\n",
    "    \"\"\"Model for the Critique Agent's structured feedback.\"\"\"\n",
    "    decision: str = Field(..., description=\"The decision, either 'IMPROVE' or 'FINAL'.\")\n",
    "    feedback: str = Field(..., description=\"Natural language feedback explaining the decision.\")\n",
    "\n",
    "\n",
    "# --- The Orchestrating Agent Class ---\n",
    "\n",
    "class SparqlRefinementAgent:\n",
    "    \"\"\"\n",
    "    An agent that orchestrates a conversation between a Query Writer and a Critique Agent\n",
    "    to iteratively develop, evaluate, and log a SPARQL query.\n",
    "    Can query a remote SPARQL endpoint or a local TTL file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sparql_endpoint: str, model_name: str = \"openai/o4-mini\", max_iterations: int = 5):\n",
    "        self.sparql_endpoint_url = sparql_endpoint\n",
    "        self.model_name = model_name\n",
    "        self.max_iterations = max_iterations\n",
    "        self.client = client\n",
    "        self.prompt_tokens = 0\n",
    "        self.completion_tokens = 0\n",
    "        self.total_tokens = 0\n",
    "        \n",
    "        # --- Differentiate between remote endpoint and local file ---\n",
    "        self.graph = None\n",
    "        self.is_remote = sparql_endpoint.lower().startswith(\"http\")\n",
    "\n",
    "        if self.is_remote:\n",
    "            print(f\"üåê Remote SPARQL endpoint mode activated: {self.sparql_endpoint_url}\")\n",
    "        else:\n",
    "            print(f\"üóÇÔ∏è Local TTL file mode activated. Loading graph from: {self.sparql_endpoint_url}\")\n",
    "            if not os.path.exists(self.sparql_endpoint_url):\n",
    "                 print(f\"   -> ‚ùå ERROR: File not found at {self.sparql_endpoint_url}. Queries will fail.\")\n",
    "                 return\n",
    "            try:\n",
    "                self.graph = Graph()\n",
    "                self.graph.parse(self.sparql_endpoint_url, format=\"turtle\")\n",
    "                print(f\"   -> ‚úÖ Graph loaded successfully with {len(self.graph)} triples.\")\n",
    "            except Exception as e:\n",
    "                print(f\"   -> ‚ùå ERROR: Failed to load or parse the TTL file: {e}\")\n",
    "                self.graph = None # Ensure graph is None on failure\n",
    "\n",
    "    def _format_rdflib_results(self, qres) -> Dict[str, Any]:\n",
    "        \"\"\"Converts rdflib QueryResult to the same dict format as SPARQLWrapper.\"\"\"\n",
    "        variables = [str(v) for v in qres.vars]\n",
    "        bindings = []\n",
    "        for row in qres:\n",
    "            binding_row = {}\n",
    "            for var_name in variables:\n",
    "                term = row[var_name]\n",
    "                if term is None:\n",
    "                    continue\n",
    "                \n",
    "                term_dict = {}\n",
    "                if isinstance(term, URIRef):\n",
    "                    term_dict = {'type': 'uri', 'value': str(term)}\n",
    "                elif isinstance(term, Literal):\n",
    "                    term_dict = {'type': 'literal', 'value': str(term)}\n",
    "                    if term.datatype:\n",
    "                        term_dict['datatype'] = str(term.datatype)\n",
    "                    if term.language:\n",
    "                        term_dict['xml:lang'] = term.language\n",
    "                elif isinstance(term, BNode):\n",
    "                    term_dict = {'type': 'bnode', 'value': str(term)}\n",
    "                \n",
    "                binding_row[var_name] = term_dict\n",
    "            bindings.append(binding_row)\n",
    "        \n",
    "        return {\"results\": bindings, \"variables\": variables}\n",
    "\n",
    "    def _run_sparql_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Executes a SPARQL query, dispatching to rdflib (local) or SPARQLWrapper (remote).\n",
    "        Returns a structured dictionary of results.\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîé Running SPARQL query... (first 80 chars: {query[:80].replace(chr(10), ' ')}...)\")\n",
    "        \n",
    "        # --- NEW: Branch for local RDF file (rdflib) ---\n",
    "        if not self.is_remote:\n",
    "            if self.graph is None:\n",
    "                return {\"summary_string\": \"SPARQL query failed: The local RDF graph is not loaded.\", \"results\": [], \"row_count\": 0, \"col_count\": 0, \"syntax_ok\": False, \"error_message\": \"Graph not loaded.\"}\n",
    "            \n",
    "            try:\n",
    "                qres = self.graph.query(query)\n",
    "                formatted_results = self._format_rdflib_results(qres)\n",
    "                bindings = formatted_results[\"results\"]\n",
    "                summary = f\"Query executed successfully on local graph. Found {len(bindings)} results.\"\n",
    "                if not bindings:\n",
    "                    summary = \"The query executed successfully on the local graph but returned no results.\"\n",
    "                \n",
    "                return {\"summary_string\": summary, \"results\": bindings, \"row_count\": len(bindings), \"col_count\": len(formatted_results[\"variables\"]), \"syntax_ok\": True, \"error_message\": None}\n",
    "            except (ParseException, Exception) as e:\n",
    "                print(f\"   -> SPARQL Query (local) Failed: {e}\")\n",
    "                error_msg = f\"The query failed to parse with the following error: {str(e)}\"\n",
    "                return {\"summary_string\": error_msg, \"results\": [], \"row_count\": 0, \"col_count\": 0, \"syntax_ok\": False, \"error_message\": str(e)}\n",
    "\n",
    "        # --- Original logic for remote SPARQL endpoint ---\n",
    "        else:\n",
    "            try:\n",
    "                sparql = SPARQLWrapper(self.sparql_endpoint_url)\n",
    "                sparql.setQuery(query)\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                results_json = sparql.query().convert()\n",
    "                \n",
    "                bindings = results_json.get(\"results\", {}).get(\"bindings\", [])\n",
    "                variables = results_json.get(\"head\", {}).get(\"vars\", [])\n",
    "                \n",
    "                summary = \"Query executed successfully. Here are the first 10 results:\\n\" + json.dumps(bindings[:10], indent=2)\n",
    "                if not bindings:\n",
    "                    summary = \"The query executed successfully but returned no results.\"\n",
    "\n",
    "                return {\"summary_string\": summary, \"results\": bindings, \"row_count\": len(bindings), \"col_count\": len(variables), \"syntax_ok\": True, \"error_message\": None}\n",
    "            except Exception as e:\n",
    "                print(f\"   -> SPARQL Query (remote) Failed: {e}\")\n",
    "                return {\"summary_string\": f\"The query failed to execute with the following error: {str(e)}\", \"results\": [], \"row_count\": 0, \"col_count\": 0, \"syntax_ok\": False, \"error_message\": str(e)}\n",
    "\n",
    "    def _get_structured_response(self, messages: List[Dict], response_model) -> Optional[BaseModel]:\n",
    "        \"\"\"Generic helper to call the LLM, parse its response, and count tokens.\"\"\"\n",
    "        content = \"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0,\n",
    "            )\n",
    "            if response.usage:\n",
    "                self.prompt_tokens += response.usage.prompt_tokens\n",
    "                self.completion_tokens += response.usage.completion_tokens\n",
    "                self.total_tokens += response.usage.total_tokens\n",
    "            \n",
    "            content = response.choices[0].message.content\n",
    "            return response_model.model_validate_json(content)\n",
    "        except (ValidationError, json.JSONDecodeError) as e:\n",
    "            print(f\"Pydantic/JSON validation failed: {e}\")\n",
    "            print(f\"--- Failing LLM Response ---\\n{content}\\n-----------------------------\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during LLM call: {e}\")\n",
    "            return None\n",
    "\n",
    "    def refine_and_evaluate_query(self, eval_data: Dict[str, Any], logger: CsvLogger, prefixes: str, knowledge_graph_content: str) -> None:\n",
    "        \"\"\"\n",
    "        The main orchestration loop to refine, evaluate, and log a query.\n",
    "        \"\"\"\n",
    "        self.prompt_tokens = 0\n",
    "        self.completion_tokens = 0\n",
    "        self.total_tokens = 0\n",
    "\n",
    "        nl_question = eval_data['question']\n",
    "        ground_truth_sparql = eval_data.get('ground_truth_sparql')\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting refinement workflow for question: '{nl_question}'\")\n",
    "\n",
    "        system_prompt = (\n",
    "            f\"You are an expert SPARQL developer for Brick Schema and ASHRAE 223p. \"\n",
    "            f\"Your job is to write a single, complete SPARQL query to answer the user's request. \"\n",
    "            f\"Here is the knowledge graph you must query:\\n\\n\"\n",
    "            f\"```turtle\\n{knowledge_graph_content}\\n```\\n\\n\"\n",
    "            f\"If you are given feedback on a prior attempt, use it to revise and improve your query. \"\n",
    "            f\"Respond ONLY with a JSON object containing the key 'sparql_query'.\"\n",
    "        )\n",
    "        \n",
    "        query_writer_messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Please write a SPARQL query that answers the following question: {nl_question}\"}\n",
    "        ]\n",
    "\n",
    "        final_generated_query = \"\"\n",
    "\n",
    "        for i in range(self.max_iterations):\n",
    "            print(f\"\\n--- Iteration {i + 1} ---\")\n",
    "            print(\"‚úçÔ∏è  Calling Query Writer Agent...\")\n",
    "            query_response = self._get_structured_response(query_writer_messages, SparqlQuery)\n",
    "            \n",
    "            if not query_response or not query_response.sparql_query:\n",
    "                print(\"‚ùå Query Writer failed to produce a valid query. Aborting iteration.\")\n",
    "                break\n",
    "            \n",
    "            final_generated_query = query_response.sparql_query\n",
    "            print(f\"   -> Query received:\\n{final_generated_query}\")\n",
    "            query_writer_messages.append({\"role\": \"assistant\", \"content\": json.dumps(query_response.model_dump())})\n",
    "\n",
    "            results_obj = self._run_sparql_query(final_generated_query)\n",
    "            print(f\"   -> Results Summary: {results_obj['summary_string'][:250]}...\")\n",
    "\n",
    "            print(\"üßê Calling Critique Agent...\")\n",
    "            critique_prompt = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in SPARQL especially for Brick Schema and ASHRAE 223p. Your job is to review a SPARQL query and its results based on an original question. Decide if the query is correct or needs improvement. Respond with a JSON object: `{\\\"decision\\\": \\\"FINAL\\\" | \\\"IMPROVE\\\", \\\"feedback\\\": \\\"...your reasoning...\\\"}`.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Original Question: \\\"{nl_question}\\\"\\n\\nSPARQL Query Attempt:\\n```sparql\\n{final_generated_query}\\n```\\n\\nExecution Results Summary:\\n{results_obj['summary_string']}\"}\n",
    "            ]\n",
    "            critique = self._get_structured_response(critique_prompt, QueryCritique)\n",
    "\n",
    "            if not critique:\n",
    "                print(\"‚ùå Critique Agent failed. Ending refinement loop.\")\n",
    "                break\n",
    "            \n",
    "            print(f\"   -> Critique Decision: {critique.decision}\")\n",
    "            print(f\"   -> Critique Feedback: {critique.feedback}\")\n",
    "\n",
    "            if critique.decision == \"FINAL\":\n",
    "                print(\"\\n‚úÖ Critique Agent approved the query. Refinement complete.\")\n",
    "                break\n",
    "            \n",
    "            feedback_for_writer = f\"Your last query attempt received the following feedback: '{critique.feedback}'. Please provide a new, improved query that addresses this feedback.\"\n",
    "            query_writer_messages.append({\"role\": \"user\", \"content\": feedback_for_writer})\n",
    "        \n",
    "        if not final_generated_query:\n",
    "            print(\"üíî Agentic workflow could not produce a final query.\")\n",
    "            return\n",
    "\n",
    "\n",
    "        print(\"\\n--- Final Evaluation and Logging ---\")\n",
    "        gen_results_obj = self._run_sparql_query(final_generated_query)\n",
    "        gt_results_obj = self._run_sparql_query(ground_truth_sparql) if ground_truth_sparql else None\n",
    "        \n",
    "        # Initialize metrics to default values\n",
    "        arity_f1, entity_set_f1, row_matching_f1, exact_match_f1 = 0.0, 0.0, 0.0, 0.0\n",
    "        less_columns_flag = False\n",
    "        \n",
    "        # Calculate metrics only if both ground truth and generated queries are valid\n",
    "        if gt_results_obj and gt_results_obj[\"syntax_ok\"] and gen_results_obj[\"syntax_ok\"]:\n",
    "            gold_rows = gt_results_obj[\"results\"]\n",
    "            pred_rows = gen_results_obj[\"results\"]\n",
    "            \n",
    "            arity_f1 = get_arity_matching_f1(gold_rows, pred_rows)\n",
    "            entity_set_f1 = get_entity_set_f1(gold_rows=gold_rows, pred_rows=pred_rows)\n",
    "            row_matching_f1 = get_row_matching_f1(gold_rows=gold_rows, pred_rows=pred_rows)\n",
    "            exact_match_f1 = get_exact_match_f1(gold_rows=gold_rows, pred_rows=pred_rows)\n",
    "            \n",
    "            # Determine if the generated query returned fewer columns than the ground truth\n",
    "            less_columns_flag = gen_results_obj['col_count'] < gt_results_obj['col_count']\n",
    "        \n",
    "        log_entry = {\n",
    "            **eval_data,\n",
    "            'model': self.model_name,\n",
    "            'generated_sparql': final_generated_query,\n",
    "            'syntax_ok': gen_results_obj['syntax_ok'],\n",
    "            'returns_results': gen_results_obj['row_count'] > 0,\n",
    "            'perfect_match': row_matching_f1 == 1.0, # Row Matching F1 is the best indicator for this\n",
    "            'gt_num_rows': gt_results_obj['row_count'] if gt_results_obj else 0,\n",
    "            'gt_num_cols': gt_results_obj['col_count'] if gt_results_obj else 0,\n",
    "            'gen_num_rows': gen_results_obj['row_count'],\n",
    "            'gen_num_cols': gen_results_obj['col_count'],\n",
    "            'arity_matching_f1': arity_f1,\n",
    "            'entity_set_f1': entity_set_f1,\n",
    "            'row_matching_f1': row_matching_f1,\n",
    "            'exact_match_f1': exact_match_f1,\n",
    "            'less_columns_flag': less_columns_flag,\n",
    "            'prompt_tokens': self.prompt_tokens,\n",
    "            'completion_tokens': self.completion_tokens,\n",
    "            'total_tokens': self.total_tokens\n",
    "        }\n",
    "        \n",
    "        logger.log(log_entry)\n",
    "        print(f\"üìä Log entry saved for query_id: {eval_data['query_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41989b",
   "metadata": {},
   "source": [
    "### 4. Define a helper function for testing a single question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "895d38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_single_question():\n",
    "    # The script will automatically detect if the target is a file path or a URL\n",
    "    SPARQL_TARGET = LOCAL_TTL_PATH if USE_LOCAL_TTL_FILE else REMOTE_ENDPOINT_URL\n",
    "\n",
    "    BRICK_PREFIXES = extract_prefixes_from_ttl(LOCAL_TTL_PATH)\n",
    "\n",
    "    KNOWLEDGE_GRAPH_CONTENT = get_kg_subset_content(LOCAL_TTL_PATH, max_triples= num_triples)\n",
    "    print(\"First 1000 chars of knowledge graph content:\")\n",
    "    print(KNOWLEDGE_GRAPH_CONTENT[:1000])\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        all_data = json.load(f)\n",
    "    target_building_data = all_data[0]\n",
    "\n",
    "    if not os.path.exists(os.path.dirname(LOG_FILE)):\n",
    "        os.makedirs(os.path.dirname(LOG_FILE))\n",
    "\n",
    "    logger = CsvLogger(filename=LOG_FILE, fieldnames=LOG_FIELDNAMES)\n",
    "    agent = SparqlRefinementAgent(\n",
    "        sparql_endpoint=SPARQL_TARGET, \n",
    "        model_name=MODEL_NAME, \n",
    "        max_iterations=3\n",
    "    )\n",
    "\n",
    "    print(f\"--- Processing building: {BUILDING_NAME} for model: {MODEL_NAME} ---\")\n",
    "\n",
    "    first_question_processed = False\n",
    "\n",
    "    try:\n",
    "        for query_info in target_building_data.get('queries', []):\n",
    "            query_id = query_info.get('query_id')\n",
    "            ground_truth_sparql = query_info.get('sparql_query')\n",
    "\n",
    "            if not ground_truth_sparql:\n",
    "                print(f\"‚ö†Ô∏è Warning: Skipping Query Group ID {query_id} (no ground truth SPARQL).\")\n",
    "                continue\n",
    "                \n",
    "            if \"prefix\" not in ground_truth_sparql.lower():\n",
    "                ground_truth_sparql = BRICK_PREFIXES + ground_truth_sparql\n",
    "\n",
    "            print(f\"\\n--- Processing Query Group ID: {query_id} ---\")\n",
    "            \n",
    "            for question_obj in query_info.get('questions', []):\n",
    "                question_text = question_obj.get('text')\n",
    "                if not question_text:\n",
    "                    continue\n",
    "\n",
    "                if check_if_question_exists(question_text, LOG_FILE, MODEL_NAME):\n",
    "                    print(f\"Question '{question_text[:50]}...' already logged. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                eval_data = {\n",
    "                    'query_id': query_id,\n",
    "                    'question_number': question_obj.get('question_number', 'N/A'),\n",
    "                    'source': question_obj.get('source', 'N/A'),\n",
    "                    'question': question_text,\n",
    "                    'ground_truth_sparql': ground_truth_sparql\n",
    "                }\n",
    "                \n",
    "                agent.refine_and_evaluate_query(\n",
    "                    eval_data=eval_data, \n",
    "                    logger=logger, \n",
    "                    prefixes=BRICK_PREFIXES,\n",
    "                    knowledge_graph_content=KNOWLEDGE_GRAPH_CONTENT\n",
    "                )\n",
    "                \n",
    "                # --- Set flag and break after the first processed question ---\n",
    "                print(\"\\nFirst question processed. Exiting loops.\")\n",
    "                first_question_processed = True\n",
    "                break # Exit the inner (questions) loop\n",
    "            # --- Check the flag to exit the outer loop as well ---\n",
    "            if first_question_processed:\n",
    "                break # Exit the outer (queries) loop\n",
    "\n",
    "    finally:\n",
    "        logger.close()\n",
    "        print(\"Logger closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006cfa4e",
   "metadata": {},
   "source": [
    "###  5.Configure the necessary test conditions: \n",
    "Set key parameters such as the target building (`BUILDING_NAME`), LLM (`MODEL_NAME`), and SPARQL endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9cf5c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully extracted 23 prefixes from bldg11.ttl.\n",
      "üîé Original graph 'bldg11.ttl' contains 62578 triples.\n",
      "   -> Graph is larger than 100 triples. Creating a subset for the prompt...\n",
      "   -> ‚úÖ Successfully created subset context with 100 triples.\n",
      "First 1000 chars of knowledge graph content:\n",
      "@prefix brick: <https://brickschema.org/schema/Brick#> .\n",
      "@prefix ns2: <http://buildsys.org/ontologies/bldg11#> .\n",
      "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
      "@prefix qudtqk: <http://qudt.org/vocab/quantitykind/> .\n",
      "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix rec: <https://w3id.org/rec#> .\n",
      "@prefix ref: <https://brickschema.org/schema/Brick/ref#> .\n",
      "@prefix sh: <http://www.w3.org/ns/shacl#> .\n",
      "@prefix skos: <http://www.w3.org/2004/02/skos/core#> .\n",
      "@prefix tag: <https://brickschema.org/schema/BrickTag#> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "ns2:AHU01 brick:isFedBy ns2:chiller .\n",
      "\n",
      "ns2:RM3530B a brick:HVAC_Zone .\n",
      "\n",
      "ns2:VAVRM1202 a brick:VAV .\n",
      "\n",
      "ns2:VAVRM2515 brick:hasPoint ns2:bldg11.ZONE.AHU05.RM2515.Zone_Air_Temp_Setpoint .\n",
      "\n",
      "ns2:VAVRM3617A brick:hasPoint ns2:bldg11.ZONE.AHU01.RM3617A.VS_2_REHEAT .\n",
      "\n",
      "ns2:VAVRM5608 brick:feeds ns2:RM5608 .\n",
      "\n",
      "ns2:VAVRM6404A brick:hasPoint ns2:bldg11.ZONE.AHU02.RM6404A\n",
      "üìù Logger initialized. Appending to Example_Results/ReAct(w100)_bldg11.csv\n",
      "üåê Remote SPARQL endpoint mode activated: http://Ozans-MacBook-Pro-9.local:7200/repositories/bldg11\n",
      "--- Processing building: bldg11 for model: openai/o3-mini ---\n",
      "\n",
      "--- Processing Query Group ID: MORTAR_001 ---\n",
      "‚úÖ Result for question 'What are the brick points in this graph?...' and model 'openai/o3-mini' already exists. Skipping.\n",
      "Question 'What are the brick points in this graph?...' already logged. Skipping.\n",
      "\n",
      "üöÄ Starting refinement workflow for question: 'Can you find nodes that has information about an external reference?'\n",
      "\n",
      "--- Iteration 1 ---\n",
      "‚úçÔ∏è  Calling Query Writer Agent...\n",
      "   -> Query received:\n",
      "PREFIX ref: <https://brickschema.org/schema/Brick/ref#>\n",
      "PREFIX ns2: <http://buildsys.org/ontologies/bldg11#>\n",
      "\n",
      "SELECT DISTINCT ?node ?externalReference\n",
      "WHERE {\n",
      "  ?node ref:hasExternalReference ?externalReference .\n",
      "}\n",
      "\n",
      "üîé Running SPARQL query... (first 80 chars: PREFIX ref: <https://brickschema.org/schema/Brick/ref#> PREFIX ns2: <http://buil...)\n",
      "   -> Results Summary: Query executed successfully. Here are the first 10 results:\n",
      "[\n",
      "  {\n",
      "    \"node\": {\n",
      "      \"type\": \"uri\",\n",
      "      \"value\": \"http://buildsys.org/ontologies/bldg11#bldg11.AHU.AHU01.CCV\"\n",
      "    },\n",
      "    \"externalReference\": {\n",
      "      \"type\": \"bnode\",\n",
      "      \"value\": \"...\n",
      "üßê Calling Critique Agent...\n",
      "   -> Critique Decision: FINAL\n",
      "   -> Critique Feedback: The SPARQL query correctly retrieves nodes that have an external reference by checking for the predicate ref:hasExternalReference. It uses a direct triple pattern which matches the original question asking for nodes with information about an external reference. The query's use of DISTINCT and the provided prefixes aligns with the Brick Schema conventions. The published results showing nodes with a blank node for external references confirm that the query is functioning as expected. Thus, no improvements are necessary.\n",
      "\n",
      "‚úÖ Critique Agent approved the query. Refinement complete.\n",
      "\n",
      "--- Final Evaluation and Logging ---\n",
      "\n",
      "üîé Running SPARQL query... (first 80 chars: PREFIX ref: <https://brickschema.org/schema/Brick/ref#> PREFIX ns2: <http://buil...)\n",
      "\n",
      "üîé Running SPARQL query... (first 80 chars: PREFIX bacnet: <http://data.ashrae.org/bacnet/2020#> PREFIX brick: <https://bric...)\n",
      "üìä Log entry saved for query_id: MORTAR_001\n",
      "\n",
      "First question processed. Exiting loops.\n",
      "‚úÖ Logger closed. Final results saved to Example_Results/ReAct(w100)_bldg11.csv\n",
      "Logger closed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME = \"openai/o3-mini\"\n",
    "BUILDING_NAME = \"bldg11\"\n",
    "num_triples = 100\n",
    "USE_LOCAL_TTL_FILE = False # We are using GraphDB for running queries. Convert this to True if you want to run queries locally.\n",
    "LOCAL_TTL_PATH = f\"./eval_buildings/{BUILDING_NAME}.ttl\"\n",
    "REMOTE_ENDPOINT_URL = f\"http://Ozans-MacBook-Pro-9.local:7200/repositories/{BUILDING_NAME}\" \n",
    "json_file_path = f\"./Benchmark_QA_pairs/{BUILDING_NAME}_combined.json\"\n",
    "LOG_FILE = f\"Example_Results/ReAct(w{num_triples})_{BUILDING_NAME}.csv\"\n",
    "\n",
    "run_single_question()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaecdeb",
   "metadata": {},
   "source": [
    "### 6. Process All Questions and Buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026fe8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_buildings(model_name: str, num_triples: int):\n",
    "    # --- Configure your details here ---\n",
    "    for BUILDING_NAME in [\"b59\", \"bldg11\",\"TUC_building\",\"dflexlibs_multizone\"]:  \n",
    "        MODEL_NAME = model_name\n",
    "        # The script will automatically detect if the target is a file path or a URL\n",
    "        SPARQL_TARGET = LOCAL_TTL_PATH if USE_LOCAL_TTL_FILE else REMOTE_ENDPOINT_URL\n",
    "\n",
    "        LOG_FILE = f\"Results/ReAct(w{num_triples})_{BUILDING_NAME}.csv\"\n",
    "\n",
    "        # --- Dynamically get prefixes from the building's TTL file ---\n",
    "        # This now correctly points to the local ttl file regardless of the query target.\n",
    "        BRICK_PREFIXES = extract_prefixes_from_ttl(LOCAL_TTL_PATH)\n",
    "        if not BRICK_PREFIXES:\n",
    "            print(\"Could not extract prefixes. Exiting.\")\n",
    "            exit()\n",
    "        \n",
    "        KNOWLEDGE_GRAPH_CONTENT = get_kg_subset_content(LOCAL_TTL_PATH, max_triples= num_triples)\n",
    "        print(\"First 1000 chars of knowledge graph content:\")\n",
    "        print(KNOWLEDGE_GRAPH_CONTENT[:1000])\n",
    "\n",
    "        \n",
    "        try:\n",
    "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                all_data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå Error: The file '{json_file_path}' was not found.\")\n",
    "            exit()\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ùå Error: The file '{json_file_path}' is not a valid JSON file.\")\n",
    "            exit()\n",
    "\n",
    "        if not isinstance(all_data, list) or not all_data:\n",
    "            print(f\"‚ùå Error: Expected JSON to be a non-empty list of building objects.\")\n",
    "            exit()\n",
    "        target_building_data = all_data[0]\n",
    "\n",
    "        #if log file dont exist, mkdir\n",
    "        if not os.path.exists(os.path.dirname(LOG_FILE)):\n",
    "            os.makedirs(os.path.dirname(LOG_FILE))\n",
    "\n",
    "\n",
    "        logger = CsvLogger(filename=LOG_FILE, fieldnames=LOG_FIELDNAMES)\n",
    "        agent = SparqlRefinementAgent(\n",
    "            sparql_endpoint=SPARQL_TARGET, \n",
    "            model_name=MODEL_NAME, \n",
    "            max_iterations=3\n",
    "        )\n",
    "\n",
    "        print(f\"--- Processing building: {BUILDING_NAME} for model: {MODEL_NAME} ---\")\n",
    "        \n",
    "        try:\n",
    "            for query_info in target_building_data.get('queries', []):\n",
    "                query_id = query_info.get('query_id')\n",
    "                ground_truth_sparql = query_info.get('sparql_query')\n",
    "\n",
    "                if not ground_truth_sparql:\n",
    "                    print(f\"‚ö†Ô∏è Warning: Skipping Query Group ID {query_id} (no ground truth SPARQL).\")\n",
    "                    continue\n",
    "                    \n",
    "                if \"prefix\" not in ground_truth_sparql.lower():\n",
    "                    ground_truth_sparql = BRICK_PREFIXES + ground_truth_sparql\n",
    "\n",
    "                print(f\"\\n--- Processing Query Group ID: {query_id} ---\")\n",
    "                \n",
    "                for question_obj in query_info.get('questions', []):\n",
    "                    question_text = question_obj.get('text')\n",
    "                    if not question_text:\n",
    "                        continue\n",
    "\n",
    "                    if check_if_question_exists(question_text, LOG_FILE, MODEL_NAME):\n",
    "                        continue\n",
    "\n",
    "                    eval_data = {\n",
    "                        'query_id': query_id,\n",
    "                        'question_number': question_obj.get('question_number', 'N/A'),\n",
    "                        'source': question_obj.get('source', 'N/A'),\n",
    "                        'question': question_text,\n",
    "                        'ground_truth_sparql': ground_truth_sparql\n",
    "                    }\n",
    "                    \n",
    "                    agent.refine_and_evaluate_query(\n",
    "                        eval_data=eval_data, \n",
    "                        logger=logger, \n",
    "                        prefixes=BRICK_PREFIXES,\n",
    "                        knowledge_graph_content=KNOWLEDGE_GRAPH_CONTENT\n",
    "                    )\n",
    "        \n",
    "        finally:\n",
    "            logger.close()\n",
    "\n",
    "\n",
    "for num_triples in [100, 5000]:\n",
    "    run_all_buildings(model_name=\"openai/o3-mini\", num_triples=num_triples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
